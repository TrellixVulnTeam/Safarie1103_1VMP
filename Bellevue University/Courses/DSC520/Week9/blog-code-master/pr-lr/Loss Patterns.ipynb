{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from functools import reduce\n",
    "from matplotlib import colors\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import Sampler\n",
    "\n",
    "# plotting params\n",
    "%matplotlib inline\n",
    "plt.rcParams['font.size'] = 10\n",
    "plt.rcParams['axes.labelsize'] = 10\n",
    "plt.rcParams['axes.titlesize'] = 10\n",
    "plt.rcParams['xtick.labelsize'] = 8\n",
    "plt.rcParams['ytick.labelsize'] = 8\n",
    "plt.rcParams['legend.fontsize'] = 10\n",
    "plt.rcParams['figure.titlesize'] = 12\n",
    "plt.rcParams['figure.figsize'] = (13.0, 6.0)\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "data_dir = './data/'\n",
    "plot_dir = './imgs/'\n",
    "dump_dir = './dump/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensuring reproducibility\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU = False\n",
    "\n",
    "device = torch.device(\"cuda\" if GPU else \"cpu\")\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if GPU else {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader\n",
    "\n",
    "We need to create a special dataloader for the experiment with shuffling. This is necessary because we need to keep track of each sample and shuffling loses that information.\n",
    "\n",
    "To solve this, we can:\n",
    "\n",
    "- create permutations of a list of numbers from 0 to 59,999 (the number of images in MNIST)\n",
    "- create a sampler class that takes a list and interates over it sequentially\n",
    "- at each epoch, create a dataloader with a sampler that gets fed the precomputed permutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearSampler(Sampler):\n",
    "    def __init__(self, idx):\n",
    "        self.idx = idx\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.idx)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_loader(data_dir, batch_size, permutation=None, num_workers=3, pin_memory=False):\n",
    "    normalize = transforms.Normalize(mean=(0.1307,), std=(0.3081,))\n",
    "    transform = transforms.Compose([transforms.ToTensor(), normalize])\n",
    "    dataset = MNIST(root=data_dir, train=True, download=True, transform=transform)\n",
    "    \n",
    "    sampler = None\n",
    "    if permutation is not None:\n",
    "        sampler = LinearSampler(permutation)\n",
    "\n",
    "    loader = DataLoader(\n",
    "        dataset, batch_size=batch_size,\n",
    "        shuffle=False, num_workers=num_workers,\n",
    "        pin_memory=pin_memory, sampler=sampler\n",
    "    )\n",
    "\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmallConv(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SmallConv, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        out = F.relu(F.max_pool2d(self.conv2(out), 2))\n",
    "        out = out.view(-1, 320)\n",
    "        out = F.relu(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return F.log_softmax(out, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predicted, ground_truth):\n",
    "    predicted = torch.max(predicted, 1)[1]\n",
    "    total = len(ground_truth)\n",
    "    correct = (predicted == ground_truth).sum().double()\n",
    "    acc = 100 * (correct / total)\n",
    "    return acc.item()\n",
    "\n",
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    \n",
    "    epoch_stats = []\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward pass\n",
    "        output = model(data)\n",
    "        acc = accuracy(output, target)\n",
    "        \n",
    "        # compute batch loss and gradient norm\n",
    "        losses = F.nll_loss(output, target, reduction='none')\n",
    "        indices = [batch_idx*train_loader.batch_size + i for i in range(len(data))]\n",
    "        \n",
    "        batch_stats = []\n",
    "        for i, l in zip(indices, losses):\n",
    "            batch_stats.append([i, l])\n",
    "        epoch_stats.append(batch_stats)\n",
    "            \n",
    "        # take average loss\n",
    "        loss = losses.mean()\n",
    "        \n",
    "        # backwards pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % 25 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tAcc: {:.2f}%'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "100. * batch_idx / len(train_loader), loss.item(), acc))\n",
    "\n",
    "    return epoch_stats\n",
    "\n",
    "def percentage_split(seq, percentages):\n",
    "    cdf = np.cumsum(percentages)\n",
    "    assert np.allclose(cdf[-1], 1.0)\n",
    "    stops = list(map(int, cdf * len(seq)))\n",
    "    return [seq[a:b] for a, b in zip([0]+stops, stops)]\n",
    "\n",
    "def bin_losses(all_epochs, num_quantiles=10):\n",
    "    percentile_splits = []\n",
    "    for epoch in all_epochs:\n",
    "        # sort by decreasing loss\n",
    "        sorted_loss_idx = sorted(\n",
    "            range(len(epoch)), key=lambda k: epoch[k][1], reverse=True\n",
    "        )\n",
    "        \n",
    "        # bin into 10 quantiles\n",
    "        splits = percentage_split(sorted_loss_idx, [num_quantiles/100]*num_quantiles)\n",
    "        \n",
    "        percentile_splits.append(splits)\n",
    "\n",
    "    return percentile_splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Without Shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "learning_rate = 1e-3\n",
    "mom = 0.99\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 3.633006\tAcc: 10.94%\n",
      "Train Epoch: 1 [1600/60000 (3%)]\tLoss: 1.619076\tAcc: 48.44%\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 0.823937\tAcc: 71.88%\n",
      "Train Epoch: 1 [4800/60000 (8%)]\tLoss: 0.598104\tAcc: 79.69%\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.370156\tAcc: 92.19%\n",
      "Train Epoch: 1 [8000/60000 (13%)]\tLoss: 0.533670\tAcc: 81.25%\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.291023\tAcc: 89.06%\n",
      "Train Epoch: 1 [11200/60000 (19%)]\tLoss: 0.611064\tAcc: 82.81%\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.301691\tAcc: 85.94%\n",
      "Train Epoch: 1 [14400/60000 (24%)]\tLoss: 0.171518\tAcc: 96.88%\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.311606\tAcc: 92.19%\n",
      "Train Epoch: 1 [17600/60000 (29%)]\tLoss: 0.311102\tAcc: 90.62%\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.225483\tAcc: 93.75%\n",
      "Train Epoch: 1 [20800/60000 (35%)]\tLoss: 0.140306\tAcc: 93.75%\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.154456\tAcc: 95.31%\n",
      "Train Epoch: 1 [24000/60000 (40%)]\tLoss: 0.203898\tAcc: 93.75%\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.032903\tAcc: 100.00%\n",
      "Train Epoch: 1 [27200/60000 (45%)]\tLoss: 0.218158\tAcc: 93.75%\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.071584\tAcc: 98.44%\n",
      "Train Epoch: 1 [30400/60000 (51%)]\tLoss: 0.053365\tAcc: 100.00%\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.092851\tAcc: 96.88%\n",
      "Train Epoch: 1 [33600/60000 (56%)]\tLoss: 0.023382\tAcc: 100.00%\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.239355\tAcc: 93.75%\n",
      "Train Epoch: 1 [36800/60000 (61%)]\tLoss: 0.159463\tAcc: 93.75%\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.054997\tAcc: 98.44%\n",
      "Train Epoch: 1 [40000/60000 (67%)]\tLoss: 0.179467\tAcc: 93.75%\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.061505\tAcc: 96.88%\n",
      "Train Epoch: 1 [43200/60000 (72%)]\tLoss: 0.169429\tAcc: 95.31%\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.159596\tAcc: 95.31%\n",
      "Train Epoch: 1 [46400/60000 (77%)]\tLoss: 0.468967\tAcc: 87.50%\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.086294\tAcc: 95.31%\n",
      "Train Epoch: 1 [49600/60000 (83%)]\tLoss: 0.220987\tAcc: 93.75%\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.208579\tAcc: 93.75%\n",
      "Train Epoch: 1 [52800/60000 (88%)]\tLoss: 0.264812\tAcc: 93.75%\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.046534\tAcc: 96.88%\n",
      "Train Epoch: 1 [56000/60000 (93%)]\tLoss: 0.117640\tAcc: 98.44%\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.196920\tAcc: 93.75%\n",
      "Train Epoch: 1 [59200/60000 (99%)]\tLoss: 0.011510\tAcc: 100.00%\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.074488\tAcc: 98.44%\n",
      "Train Epoch: 2 [1600/60000 (3%)]\tLoss: 0.132464\tAcc: 96.88%\n",
      "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.066270\tAcc: 98.44%\n",
      "Train Epoch: 2 [4800/60000 (8%)]\tLoss: 0.071151\tAcc: 98.44%\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.210089\tAcc: 96.88%\n",
      "Train Epoch: 2 [8000/60000 (13%)]\tLoss: 0.061824\tAcc: 98.44%\n",
      "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.054427\tAcc: 96.88%\n",
      "Train Epoch: 2 [11200/60000 (19%)]\tLoss: 0.140843\tAcc: 93.75%\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.093792\tAcc: 95.31%\n",
      "Train Epoch: 2 [14400/60000 (24%)]\tLoss: 0.045398\tAcc: 98.44%\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.158572\tAcc: 95.31%\n",
      "Train Epoch: 2 [17600/60000 (29%)]\tLoss: 0.026146\tAcc: 100.00%\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.226149\tAcc: 96.88%\n",
      "Train Epoch: 2 [20800/60000 (35%)]\tLoss: 0.018054\tAcc: 100.00%\n",
      "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.030244\tAcc: 100.00%\n",
      "Train Epoch: 2 [24000/60000 (40%)]\tLoss: 0.028830\tAcc: 98.44%\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.006937\tAcc: 100.00%\n",
      "Train Epoch: 2 [27200/60000 (45%)]\tLoss: 0.154943\tAcc: 95.31%\n",
      "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.044339\tAcc: 96.88%\n",
      "Train Epoch: 2 [30400/60000 (51%)]\tLoss: 0.041290\tAcc: 98.44%\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.060451\tAcc: 98.44%\n",
      "Train Epoch: 2 [33600/60000 (56%)]\tLoss: 0.015808\tAcc: 100.00%\n",
      "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.101732\tAcc: 96.88%\n",
      "Train Epoch: 2 [36800/60000 (61%)]\tLoss: 0.061473\tAcc: 98.44%\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.041257\tAcc: 98.44%\n",
      "Train Epoch: 2 [40000/60000 (67%)]\tLoss: 0.042018\tAcc: 98.44%\n",
      "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.039075\tAcc: 98.44%\n",
      "Train Epoch: 2 [43200/60000 (72%)]\tLoss: 0.040230\tAcc: 96.88%\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.066020\tAcc: 96.88%\n",
      "Train Epoch: 2 [46400/60000 (77%)]\tLoss: 0.359166\tAcc: 87.50%\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.056128\tAcc: 98.44%\n",
      "Train Epoch: 2 [49600/60000 (83%)]\tLoss: 0.159148\tAcc: 95.31%\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.183856\tAcc: 95.31%\n",
      "Train Epoch: 2 [52800/60000 (88%)]\tLoss: 0.201649\tAcc: 95.31%\n",
      "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.019636\tAcc: 100.00%\n",
      "Train Epoch: 2 [56000/60000 (93%)]\tLoss: 0.046323\tAcc: 98.44%\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.109396\tAcc: 96.88%\n",
      "Train Epoch: 2 [59200/60000 (99%)]\tLoss: 0.015261\tAcc: 100.00%\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.024345\tAcc: 100.00%\n",
      "Train Epoch: 3 [1600/60000 (3%)]\tLoss: 0.103422\tAcc: 98.44%\n",
      "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.014144\tAcc: 100.00%\n",
      "Train Epoch: 3 [4800/60000 (8%)]\tLoss: 0.037579\tAcc: 100.00%\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.198094\tAcc: 96.88%\n",
      "Train Epoch: 3 [8000/60000 (13%)]\tLoss: 0.014647\tAcc: 100.00%\n",
      "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.030492\tAcc: 100.00%\n",
      "Train Epoch: 3 [11200/60000 (19%)]\tLoss: 0.067932\tAcc: 96.88%\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.133189\tAcc: 96.88%\n",
      "Train Epoch: 3 [14400/60000 (24%)]\tLoss: 0.037543\tAcc: 98.44%\n",
      "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.070643\tAcc: 98.44%\n",
      "Train Epoch: 3 [17600/60000 (29%)]\tLoss: 0.008885\tAcc: 100.00%\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.181374\tAcc: 95.31%\n",
      "Train Epoch: 3 [20800/60000 (35%)]\tLoss: 0.008857\tAcc: 100.00%\n",
      "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 0.090927\tAcc: 96.88%\n",
      "Train Epoch: 3 [24000/60000 (40%)]\tLoss: 0.029201\tAcc: 98.44%\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.022435\tAcc: 98.44%\n",
      "Train Epoch: 3 [27200/60000 (45%)]\tLoss: 0.116180\tAcc: 96.88%\n",
      "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.016861\tAcc: 100.00%\n",
      "Train Epoch: 3 [30400/60000 (51%)]\tLoss: 0.034622\tAcc: 96.88%\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.081620\tAcc: 95.31%\n",
      "Train Epoch: 3 [33600/60000 (56%)]\tLoss: 0.017481\tAcc: 98.44%\n",
      "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 0.142122\tAcc: 96.88%\n",
      "Train Epoch: 3 [36800/60000 (61%)]\tLoss: 0.041635\tAcc: 98.44%\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.041654\tAcc: 98.44%\n",
      "Train Epoch: 3 [40000/60000 (67%)]\tLoss: 0.027219\tAcc: 98.44%\n",
      "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 0.035152\tAcc: 98.44%\n",
      "Train Epoch: 3 [43200/60000 (72%)]\tLoss: 0.032792\tAcc: 96.88%\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.037414\tAcc: 98.44%\n",
      "Train Epoch: 3 [46400/60000 (77%)]\tLoss: 0.252676\tAcc: 90.62%\n",
      "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.022737\tAcc: 100.00%\n",
      "Train Epoch: 3 [49600/60000 (83%)]\tLoss: 0.060854\tAcc: 96.88%\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.182138\tAcc: 96.88%\n",
      "Train Epoch: 3 [52800/60000 (88%)]\tLoss: 0.145763\tAcc: 96.88%\n",
      "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 0.023650\tAcc: 98.44%\n",
      "Train Epoch: 3 [56000/60000 (93%)]\tLoss: 0.021866\tAcc: 98.44%\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.099294\tAcc: 96.88%\n",
      "Train Epoch: 3 [59200/60000 (99%)]\tLoss: 0.007801\tAcc: 100.00%\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.008445\tAcc: 100.00%\n",
      "Train Epoch: 4 [1600/60000 (3%)]\tLoss: 0.108969\tAcc: 98.44%\n",
      "Train Epoch: 4 [3200/60000 (5%)]\tLoss: 0.012685\tAcc: 100.00%\n",
      "Train Epoch: 4 [4800/60000 (8%)]\tLoss: 0.051898\tAcc: 98.44%\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.218408\tAcc: 96.88%\n",
      "Train Epoch: 4 [8000/60000 (13%)]\tLoss: 0.004244\tAcc: 100.00%\n",
      "Train Epoch: 4 [9600/60000 (16%)]\tLoss: 0.048583\tAcc: 96.88%\n",
      "Train Epoch: 4 [11200/60000 (19%)]\tLoss: 0.053676\tAcc: 96.88%\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.119289\tAcc: 96.88%\n",
      "Train Epoch: 4 [14400/60000 (24%)]\tLoss: 0.014504\tAcc: 100.00%\n",
      "Train Epoch: 4 [16000/60000 (27%)]\tLoss: 0.034672\tAcc: 100.00%\n",
      "Train Epoch: 4 [17600/60000 (29%)]\tLoss: 0.015773\tAcc: 100.00%\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.146741\tAcc: 96.88%\n",
      "Train Epoch: 4 [20800/60000 (35%)]\tLoss: 0.004331\tAcc: 100.00%\n",
      "Train Epoch: 4 [22400/60000 (37%)]\tLoss: 0.058988\tAcc: 96.88%\n",
      "Train Epoch: 4 [24000/60000 (40%)]\tLoss: 0.023140\tAcc: 98.44%\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.005246\tAcc: 100.00%\n",
      "Train Epoch: 4 [27200/60000 (45%)]\tLoss: 0.053999\tAcc: 96.88%\n",
      "Train Epoch: 4 [28800/60000 (48%)]\tLoss: 0.005452\tAcc: 100.00%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 4 [30400/60000 (51%)]\tLoss: 0.011213\tAcc: 100.00%\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.038986\tAcc: 96.88%\n",
      "Train Epoch: 4 [33600/60000 (56%)]\tLoss: 0.011563\tAcc: 100.00%\n",
      "Train Epoch: 4 [35200/60000 (59%)]\tLoss: 0.129776\tAcc: 96.88%\n",
      "Train Epoch: 4 [36800/60000 (61%)]\tLoss: 0.034880\tAcc: 98.44%\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.020573\tAcc: 100.00%\n",
      "Train Epoch: 4 [40000/60000 (67%)]\tLoss: 0.011226\tAcc: 100.00%\n",
      "Train Epoch: 4 [41600/60000 (69%)]\tLoss: 0.037513\tAcc: 98.44%\n",
      "Train Epoch: 4 [43200/60000 (72%)]\tLoss: 0.058901\tAcc: 98.44%\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.043138\tAcc: 98.44%\n",
      "Train Epoch: 4 [46400/60000 (77%)]\tLoss: 0.187012\tAcc: 93.75%\n",
      "Train Epoch: 4 [48000/60000 (80%)]\tLoss: 0.007739\tAcc: 100.00%\n",
      "Train Epoch: 4 [49600/60000 (83%)]\tLoss: 0.029716\tAcc: 100.00%\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.147594\tAcc: 98.44%\n",
      "Train Epoch: 4 [52800/60000 (88%)]\tLoss: 0.112262\tAcc: 96.88%\n",
      "Train Epoch: 4 [54400/60000 (91%)]\tLoss: 0.010896\tAcc: 100.00%\n",
      "Train Epoch: 4 [56000/60000 (93%)]\tLoss: 0.019521\tAcc: 98.44%\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.133354\tAcc: 95.31%\n",
      "Train Epoch: 4 [59200/60000 (99%)]\tLoss: 0.002342\tAcc: 100.00%\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.021062\tAcc: 98.44%\n",
      "Train Epoch: 5 [1600/60000 (3%)]\tLoss: 0.079082\tAcc: 98.44%\n",
      "Train Epoch: 5 [3200/60000 (5%)]\tLoss: 0.022650\tAcc: 98.44%\n",
      "Train Epoch: 5 [4800/60000 (8%)]\tLoss: 0.056475\tAcc: 98.44%\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.196984\tAcc: 96.88%\n",
      "Train Epoch: 5 [8000/60000 (13%)]\tLoss: 0.002361\tAcc: 100.00%\n",
      "Train Epoch: 5 [9600/60000 (16%)]\tLoss: 0.031388\tAcc: 100.00%\n",
      "Train Epoch: 5 [11200/60000 (19%)]\tLoss: 0.030519\tAcc: 98.44%\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.093129\tAcc: 96.88%\n",
      "Train Epoch: 5 [14400/60000 (24%)]\tLoss: 0.021665\tAcc: 100.00%\n",
      "Train Epoch: 5 [16000/60000 (27%)]\tLoss: 0.043810\tAcc: 96.88%\n",
      "Train Epoch: 5 [17600/60000 (29%)]\tLoss: 0.007854\tAcc: 100.00%\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.133513\tAcc: 96.88%\n",
      "Train Epoch: 5 [20800/60000 (35%)]\tLoss: 0.003202\tAcc: 100.00%\n",
      "Train Epoch: 5 [22400/60000 (37%)]\tLoss: 0.033618\tAcc: 98.44%\n",
      "Train Epoch: 5 [24000/60000 (40%)]\tLoss: 0.013201\tAcc: 100.00%\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.002040\tAcc: 100.00%\n",
      "Train Epoch: 5 [27200/60000 (45%)]\tLoss: 0.046378\tAcc: 98.44%\n",
      "Train Epoch: 5 [28800/60000 (48%)]\tLoss: 0.004634\tAcc: 100.00%\n",
      "Train Epoch: 5 [30400/60000 (51%)]\tLoss: 0.009121\tAcc: 100.00%\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.047612\tAcc: 98.44%\n",
      "Train Epoch: 5 [33600/60000 (56%)]\tLoss: 0.006736\tAcc: 100.00%\n",
      "Train Epoch: 5 [35200/60000 (59%)]\tLoss: 0.138957\tAcc: 96.88%\n",
      "Train Epoch: 5 [36800/60000 (61%)]\tLoss: 0.037154\tAcc: 96.88%\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.025383\tAcc: 98.44%\n",
      "Train Epoch: 5 [40000/60000 (67%)]\tLoss: 0.017236\tAcc: 100.00%\n",
      "Train Epoch: 5 [41600/60000 (69%)]\tLoss: 0.039191\tAcc: 98.44%\n",
      "Train Epoch: 5 [43200/60000 (72%)]\tLoss: 0.048996\tAcc: 98.44%\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.043573\tAcc: 98.44%\n",
      "Train Epoch: 5 [46400/60000 (77%)]\tLoss: 0.159468\tAcc: 95.31%\n",
      "Train Epoch: 5 [48000/60000 (80%)]\tLoss: 0.007549\tAcc: 100.00%\n",
      "Train Epoch: 5 [49600/60000 (83%)]\tLoss: 0.023951\tAcc: 100.00%\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.146604\tAcc: 98.44%\n",
      "Train Epoch: 5 [52800/60000 (88%)]\tLoss: 0.139337\tAcc: 95.31%\n",
      "Train Epoch: 5 [54400/60000 (91%)]\tLoss: 0.002421\tAcc: 100.00%\n",
      "Train Epoch: 5 [56000/60000 (93%)]\tLoss: 0.013481\tAcc: 100.00%\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.161190\tAcc: 95.31%\n",
      "Train Epoch: 5 [59200/60000 (99%)]\tLoss: 0.002633\tAcc: 100.00%\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(SEED)\n",
    "\n",
    "# instantiate convnet\n",
    "model = SmallConv().to(device)\n",
    "\n",
    "# relu init\n",
    "for m in model.modules():\n",
    "    if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "        nn.init.kaiming_normal_(m.weight, mode='fan_in')\n",
    "\n",
    "# define optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=mom)\n",
    "\n",
    "# instantiate data loader\n",
    "train_loader = get_data_loader(data_dir, batch_size, None, **kwargs)\n",
    "\n",
    "stats_no_shuffling = []\n",
    "for epoch in range(1,  num_epochs+1):\n",
    "    stats_no_shuffling.append(train(model, device, train_loader, optimizer, epoch))\n",
    "pickle.dump(stats_no_shuffling, open(dump_dir + \"no_shuffling.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With Shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "learning_rate = 1e-3\n",
    "mom = 0.99\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create permutations\n",
    "permutations = []\n",
    "permutations.append(list(np.arange(60000)))\n",
    "\n",
    "x = list(np.arange(60000))\n",
    "np.random.seed(SEED)\n",
    "\n",
    "for _ in range(num_epochs-1):\n",
    "    np.random.shuffle(x)\n",
    "    permutations.append(x.copy())\n",
    "pickle.dump(permutations, open(dump_dir + \"permutations.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 3.633006\tAcc: 10.94%\n",
      "Train Epoch: 1 [1600/60000 (3%)]\tLoss: 1.619076\tAcc: 48.44%\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 0.823937\tAcc: 71.88%\n",
      "Train Epoch: 1 [4800/60000 (8%)]\tLoss: 0.598104\tAcc: 79.69%\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.370156\tAcc: 92.19%\n",
      "Train Epoch: 1 [8000/60000 (13%)]\tLoss: 0.533670\tAcc: 81.25%\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.291023\tAcc: 89.06%\n",
      "Train Epoch: 1 [11200/60000 (19%)]\tLoss: 0.611064\tAcc: 82.81%\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.301691\tAcc: 85.94%\n",
      "Train Epoch: 1 [14400/60000 (24%)]\tLoss: 0.171518\tAcc: 96.88%\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.311606\tAcc: 92.19%\n",
      "Train Epoch: 1 [17600/60000 (29%)]\tLoss: 0.311102\tAcc: 90.62%\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.225483\tAcc: 93.75%\n",
      "Train Epoch: 1 [20800/60000 (35%)]\tLoss: 0.140306\tAcc: 93.75%\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.154456\tAcc: 95.31%\n",
      "Train Epoch: 1 [24000/60000 (40%)]\tLoss: 0.203898\tAcc: 93.75%\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.032903\tAcc: 100.00%\n",
      "Train Epoch: 1 [27200/60000 (45%)]\tLoss: 0.218158\tAcc: 93.75%\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.071584\tAcc: 98.44%\n",
      "Train Epoch: 1 [30400/60000 (51%)]\tLoss: 0.053365\tAcc: 100.00%\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.092851\tAcc: 96.88%\n",
      "Train Epoch: 1 [33600/60000 (56%)]\tLoss: 0.023382\tAcc: 100.00%\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.239355\tAcc: 93.75%\n",
      "Train Epoch: 1 [36800/60000 (61%)]\tLoss: 0.159463\tAcc: 93.75%\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.054997\tAcc: 98.44%\n",
      "Train Epoch: 1 [40000/60000 (67%)]\tLoss: 0.179467\tAcc: 93.75%\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.061505\tAcc: 96.88%\n",
      "Train Epoch: 1 [43200/60000 (72%)]\tLoss: 0.169429\tAcc: 95.31%\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.159596\tAcc: 95.31%\n",
      "Train Epoch: 1 [46400/60000 (77%)]\tLoss: 0.468967\tAcc: 87.50%\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.086294\tAcc: 95.31%\n",
      "Train Epoch: 1 [49600/60000 (83%)]\tLoss: 0.220987\tAcc: 93.75%\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.208579\tAcc: 93.75%\n",
      "Train Epoch: 1 [52800/60000 (88%)]\tLoss: 0.264812\tAcc: 93.75%\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.046534\tAcc: 96.88%\n",
      "Train Epoch: 1 [56000/60000 (93%)]\tLoss: 0.117640\tAcc: 98.44%\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.196920\tAcc: 93.75%\n",
      "Train Epoch: 1 [59200/60000 (99%)]\tLoss: 0.011510\tAcc: 100.00%\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.102078\tAcc: 98.44%\n",
      "Train Epoch: 2 [1600/60000 (3%)]\tLoss: 0.031098\tAcc: 98.44%\n",
      "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.232979\tAcc: 93.75%\n",
      "Train Epoch: 2 [4800/60000 (8%)]\tLoss: 0.016755\tAcc: 100.00%\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.025166\tAcc: 100.00%\n",
      "Train Epoch: 2 [8000/60000 (13%)]\tLoss: 0.023412\tAcc: 100.00%\n",
      "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.036617\tAcc: 98.44%\n",
      "Train Epoch: 2 [11200/60000 (19%)]\tLoss: 0.068466\tAcc: 98.44%\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.098916\tAcc: 95.31%\n",
      "Train Epoch: 2 [14400/60000 (24%)]\tLoss: 0.026504\tAcc: 100.00%\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.090279\tAcc: 96.88%\n",
      "Train Epoch: 2 [17600/60000 (29%)]\tLoss: 0.062686\tAcc: 95.31%\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.080275\tAcc: 95.31%\n",
      "Train Epoch: 2 [20800/60000 (35%)]\tLoss: 0.059304\tAcc: 98.44%\n",
      "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.013309\tAcc: 100.00%\n",
      "Train Epoch: 2 [24000/60000 (40%)]\tLoss: 0.011628\tAcc: 100.00%\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.080991\tAcc: 96.88%\n",
      "Train Epoch: 2 [27200/60000 (45%)]\tLoss: 0.032248\tAcc: 98.44%\n",
      "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.111027\tAcc: 95.31%\n",
      "Train Epoch: 2 [30400/60000 (51%)]\tLoss: 0.114232\tAcc: 95.31%\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.030984\tAcc: 98.44%\n",
      "Train Epoch: 2 [33600/60000 (56%)]\tLoss: 0.078294\tAcc: 95.31%\n",
      "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.243407\tAcc: 96.88%\n",
      "Train Epoch: 2 [36800/60000 (61%)]\tLoss: 0.043078\tAcc: 98.44%\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.090051\tAcc: 98.44%\n",
      "Train Epoch: 2 [40000/60000 (67%)]\tLoss: 0.170737\tAcc: 96.88%\n",
      "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.125144\tAcc: 95.31%\n",
      "Train Epoch: 2 [43200/60000 (72%)]\tLoss: 0.127538\tAcc: 96.88%\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.085707\tAcc: 96.88%\n",
      "Train Epoch: 2 [46400/60000 (77%)]\tLoss: 0.049352\tAcc: 96.88%\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.120053\tAcc: 95.31%\n",
      "Train Epoch: 2 [49600/60000 (83%)]\tLoss: 0.089620\tAcc: 96.88%\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.051293\tAcc: 98.44%\n",
      "Train Epoch: 2 [52800/60000 (88%)]\tLoss: 0.039015\tAcc: 98.44%\n",
      "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.004403\tAcc: 100.00%\n",
      "Train Epoch: 2 [56000/60000 (93%)]\tLoss: 0.171666\tAcc: 95.31%\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.025525\tAcc: 98.44%\n",
      "Train Epoch: 2 [59200/60000 (99%)]\tLoss: 0.010009\tAcc: 100.00%\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.019292\tAcc: 100.00%\n",
      "Train Epoch: 3 [1600/60000 (3%)]\tLoss: 0.018023\tAcc: 100.00%\n",
      "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.083953\tAcc: 95.31%\n",
      "Train Epoch: 3 [4800/60000 (8%)]\tLoss: 0.016674\tAcc: 100.00%\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.052217\tAcc: 98.44%\n",
      "Train Epoch: 3 [8000/60000 (13%)]\tLoss: 0.022347\tAcc: 100.00%\n",
      "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.084956\tAcc: 98.44%\n",
      "Train Epoch: 3 [11200/60000 (19%)]\tLoss: 0.041385\tAcc: 98.44%\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.201479\tAcc: 92.19%\n",
      "Train Epoch: 3 [14400/60000 (24%)]\tLoss: 0.006730\tAcc: 100.00%\n",
      "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.038304\tAcc: 98.44%\n",
      "Train Epoch: 3 [17600/60000 (29%)]\tLoss: 0.042132\tAcc: 98.44%\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.137135\tAcc: 96.88%\n",
      "Train Epoch: 3 [20800/60000 (35%)]\tLoss: 0.140588\tAcc: 96.88%\n",
      "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 0.016618\tAcc: 100.00%\n",
      "Train Epoch: 3 [24000/60000 (40%)]\tLoss: 0.014411\tAcc: 100.00%\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.133534\tAcc: 95.31%\n",
      "Train Epoch: 3 [27200/60000 (45%)]\tLoss: 0.016728\tAcc: 100.00%\n",
      "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.074302\tAcc: 96.88%\n",
      "Train Epoch: 3 [30400/60000 (51%)]\tLoss: 0.081907\tAcc: 96.88%\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.204502\tAcc: 93.75%\n",
      "Train Epoch: 3 [33600/60000 (56%)]\tLoss: 0.044639\tAcc: 98.44%\n",
      "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 0.005864\tAcc: 100.00%\n",
      "Train Epoch: 3 [36800/60000 (61%)]\tLoss: 0.065540\tAcc: 96.88%\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.106513\tAcc: 98.44%\n",
      "Train Epoch: 3 [40000/60000 (67%)]\tLoss: 0.028559\tAcc: 98.44%\n",
      "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 0.258976\tAcc: 93.75%\n",
      "Train Epoch: 3 [43200/60000 (72%)]\tLoss: 0.004482\tAcc: 100.00%\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.008298\tAcc: 100.00%\n",
      "Train Epoch: 3 [46400/60000 (77%)]\tLoss: 0.086896\tAcc: 96.88%\n",
      "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.078228\tAcc: 96.88%\n",
      "Train Epoch: 3 [49600/60000 (83%)]\tLoss: 0.025244\tAcc: 98.44%\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.040387\tAcc: 98.44%\n",
      "Train Epoch: 3 [52800/60000 (88%)]\tLoss: 0.052178\tAcc: 98.44%\n",
      "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 0.035417\tAcc: 98.44%\n",
      "Train Epoch: 3 [56000/60000 (93%)]\tLoss: 0.050410\tAcc: 96.88%\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.131319\tAcc: 96.88%\n",
      "Train Epoch: 3 [59200/60000 (99%)]\tLoss: 0.064430\tAcc: 98.44%\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.089984\tAcc: 95.31%\n",
      "Train Epoch: 4 [1600/60000 (3%)]\tLoss: 0.075069\tAcc: 96.88%\n",
      "Train Epoch: 4 [3200/60000 (5%)]\tLoss: 0.037407\tAcc: 100.00%\n",
      "Train Epoch: 4 [4800/60000 (8%)]\tLoss: 0.284701\tAcc: 96.88%\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.005039\tAcc: 100.00%\n",
      "Train Epoch: 4 [8000/60000 (13%)]\tLoss: 0.049454\tAcc: 96.88%\n",
      "Train Epoch: 4 [9600/60000 (16%)]\tLoss: 0.070564\tAcc: 98.44%\n",
      "Train Epoch: 4 [11200/60000 (19%)]\tLoss: 0.061793\tAcc: 98.44%\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.043478\tAcc: 98.44%\n",
      "Train Epoch: 4 [14400/60000 (24%)]\tLoss: 0.015482\tAcc: 98.44%\n",
      "Train Epoch: 4 [16000/60000 (27%)]\tLoss: 0.063184\tAcc: 98.44%\n",
      "Train Epoch: 4 [17600/60000 (29%)]\tLoss: 0.004628\tAcc: 100.00%\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.119642\tAcc: 95.31%\n",
      "Train Epoch: 4 [20800/60000 (35%)]\tLoss: 0.064909\tAcc: 96.88%\n",
      "Train Epoch: 4 [22400/60000 (37%)]\tLoss: 0.038996\tAcc: 98.44%\n",
      "Train Epoch: 4 [24000/60000 (40%)]\tLoss: 0.034809\tAcc: 100.00%\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.009662\tAcc: 100.00%\n",
      "Train Epoch: 4 [27200/60000 (45%)]\tLoss: 0.010710\tAcc: 100.00%\n",
      "Train Epoch: 4 [28800/60000 (48%)]\tLoss: 0.014937\tAcc: 98.44%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 4 [30400/60000 (51%)]\tLoss: 0.041674\tAcc: 100.00%\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.203437\tAcc: 95.31%\n",
      "Train Epoch: 4 [33600/60000 (56%)]\tLoss: 0.010256\tAcc: 100.00%\n",
      "Train Epoch: 4 [35200/60000 (59%)]\tLoss: 0.102535\tAcc: 93.75%\n",
      "Train Epoch: 4 [36800/60000 (61%)]\tLoss: 0.030967\tAcc: 98.44%\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.050214\tAcc: 96.88%\n",
      "Train Epoch: 4 [40000/60000 (67%)]\tLoss: 0.032882\tAcc: 98.44%\n",
      "Train Epoch: 4 [41600/60000 (69%)]\tLoss: 0.174472\tAcc: 98.44%\n",
      "Train Epoch: 4 [43200/60000 (72%)]\tLoss: 0.044490\tAcc: 98.44%\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.052338\tAcc: 98.44%\n",
      "Train Epoch: 4 [46400/60000 (77%)]\tLoss: 0.005276\tAcc: 100.00%\n",
      "Train Epoch: 4 [48000/60000 (80%)]\tLoss: 0.009933\tAcc: 100.00%\n",
      "Train Epoch: 4 [49600/60000 (83%)]\tLoss: 0.200144\tAcc: 96.88%\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.048852\tAcc: 98.44%\n",
      "Train Epoch: 4 [52800/60000 (88%)]\tLoss: 0.030278\tAcc: 98.44%\n",
      "Train Epoch: 4 [54400/60000 (91%)]\tLoss: 0.054861\tAcc: 98.44%\n",
      "Train Epoch: 4 [56000/60000 (93%)]\tLoss: 0.003664\tAcc: 100.00%\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.001415\tAcc: 100.00%\n",
      "Train Epoch: 4 [59200/60000 (99%)]\tLoss: 0.009499\tAcc: 100.00%\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.013112\tAcc: 100.00%\n",
      "Train Epoch: 5 [1600/60000 (3%)]\tLoss: 0.029796\tAcc: 100.00%\n",
      "Train Epoch: 5 [3200/60000 (5%)]\tLoss: 0.003726\tAcc: 100.00%\n",
      "Train Epoch: 5 [4800/60000 (8%)]\tLoss: 0.023652\tAcc: 98.44%\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.007024\tAcc: 100.00%\n",
      "Train Epoch: 5 [8000/60000 (13%)]\tLoss: 0.029964\tAcc: 98.44%\n",
      "Train Epoch: 5 [9600/60000 (16%)]\tLoss: 0.166122\tAcc: 93.75%\n",
      "Train Epoch: 5 [11200/60000 (19%)]\tLoss: 0.005029\tAcc: 100.00%\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.070467\tAcc: 98.44%\n",
      "Train Epoch: 5 [14400/60000 (24%)]\tLoss: 0.007154\tAcc: 100.00%\n",
      "Train Epoch: 5 [16000/60000 (27%)]\tLoss: 0.118603\tAcc: 96.88%\n",
      "Train Epoch: 5 [17600/60000 (29%)]\tLoss: 0.047950\tAcc: 98.44%\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.048197\tAcc: 96.88%\n",
      "Train Epoch: 5 [20800/60000 (35%)]\tLoss: 0.024703\tAcc: 98.44%\n",
      "Train Epoch: 5 [22400/60000 (37%)]\tLoss: 0.032201\tAcc: 98.44%\n",
      "Train Epoch: 5 [24000/60000 (40%)]\tLoss: 0.010922\tAcc: 100.00%\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.033510\tAcc: 98.44%\n",
      "Train Epoch: 5 [27200/60000 (45%)]\tLoss: 0.005127\tAcc: 100.00%\n",
      "Train Epoch: 5 [28800/60000 (48%)]\tLoss: 0.019979\tAcc: 98.44%\n",
      "Train Epoch: 5 [30400/60000 (51%)]\tLoss: 0.041545\tAcc: 100.00%\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.037461\tAcc: 98.44%\n",
      "Train Epoch: 5 [33600/60000 (56%)]\tLoss: 0.039346\tAcc: 96.88%\n",
      "Train Epoch: 5 [35200/60000 (59%)]\tLoss: 0.045353\tAcc: 98.44%\n",
      "Train Epoch: 5 [36800/60000 (61%)]\tLoss: 0.011449\tAcc: 100.00%\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.032527\tAcc: 98.44%\n",
      "Train Epoch: 5 [40000/60000 (67%)]\tLoss: 0.002806\tAcc: 100.00%\n",
      "Train Epoch: 5 [41600/60000 (69%)]\tLoss: 0.001624\tAcc: 100.00%\n",
      "Train Epoch: 5 [43200/60000 (72%)]\tLoss: 0.066037\tAcc: 98.44%\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.014355\tAcc: 100.00%\n",
      "Train Epoch: 5 [46400/60000 (77%)]\tLoss: 0.002218\tAcc: 100.00%\n",
      "Train Epoch: 5 [48000/60000 (80%)]\tLoss: 0.049414\tAcc: 98.44%\n",
      "Train Epoch: 5 [49600/60000 (83%)]\tLoss: 0.022918\tAcc: 98.44%\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.091738\tAcc: 98.44%\n",
      "Train Epoch: 5 [52800/60000 (88%)]\tLoss: 0.004077\tAcc: 100.00%\n",
      "Train Epoch: 5 [54400/60000 (91%)]\tLoss: 0.001125\tAcc: 100.00%\n",
      "Train Epoch: 5 [56000/60000 (93%)]\tLoss: 0.012950\tAcc: 98.44%\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.003812\tAcc: 100.00%\n",
      "Train Epoch: 5 [59200/60000 (99%)]\tLoss: 0.002558\tAcc: 100.00%\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(SEED)\n",
    "\n",
    "model = SmallConv().to(device)\n",
    "\n",
    "# relu init\n",
    "for m in model.modules():\n",
    "    if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "        nn.init.kaiming_normal_(m.weight, mode='fan_in')\n",
    "\n",
    "# define optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=mom)\n",
    "\n",
    "stats_with_shuffling = []\n",
    "for epoch in range(1,  num_epochs+1):\n",
    "    train_loader = get_data_loader(data_dir, batch_size, permutations[epoch-1], **kwargs)\n",
    "    stats_with_shuffling.append(train(model, device, train_loader, optimizer, epoch))\n",
    "pickle.dump(stats_with_shuffling, open(dump_dir + \"with_shuffling.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Quantiles - Shuffling ON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_with_shuffling = pickle.load(open(dump_dir + \"with_shuffling.pkl\", \"rb\"))\n",
    "permutations = pickle.load(open(dump_dir + \"permutations.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten the list for each epoch\n",
    "stats_with_shuffling_flat = []\n",
    "for stat in stats_with_shuffling:\n",
    "    stats_with_shuffling_flat.append(\n",
    "        [v for sublist in stat for v in sublist]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remap the indices based on the permutations list\n",
    "for stat, perm in zip(stats_with_shuffling_flat, permutations):\n",
    "    for i in range(len(stat)):\n",
    "        stat[i][0] = perm[i]\n",
    "        stat[i][1] = stat[i][1].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resort in increasing index order\n",
    "for i in range(len(stats_with_shuffling_flat)):\n",
    "    stats_with_shuffling_flat[i] = sorted(stats_with_shuffling_flat[i], key=lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get percentile splits for all 5 epochs\n",
    "num_quantiles = 10\n",
    "percentile_splits = bin_losses(stats_with_shuffling_flat, num_quantiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fr = [0, 1, 3]\n",
    "to = [0, 0, 0]\n",
    "names = ['1 to 5', '2 to 5', '4 to 5']\n",
    "\n",
    "all_matches = []\n",
    "for f, t in zip(fr, to):\n",
    "    percent_matches = []\n",
    "    for i in range(num_quantiles):\n",
    "        percentile_all = []\n",
    "        for j in range(f, len(percentile_splits)-t):\n",
    "            percentile_all.append(percentile_splits[j][i])\n",
    "        matching = reduce(np.intersect1d, percentile_all)\n",
    "        percent = 100 * len(matching) / len(percentile_all[0])\n",
    "        percent_matches.append(percent)\n",
    "    all_matches.append(percent_matches)\n",
    "    \n",
    "        \n",
    "fig, axes = plt.subplots(1, 3, figsize=(8, 4))\n",
    "for i, (ax, match, n) in enumerate(zip(axes, all_matches, names)):\n",
    "    ax.bar(range(1, num_quantiles+1), match, width=0.9, color='b')\n",
    "    if i == 0:\n",
    "        ax.set_ylabel('Percent Match')\n",
    "    ax.set_title('Epochs {} (With Shuffling)'.format(n))\n",
    "    ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    ax.set_ylim([0, 90])\n",
    "    \n",
    "plt.savefig(plot_dir + \"with_shuffling.jpg\", format=\"jpg\", dpi=1000, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Quantiles - Shuffling OFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_no_shuffling = pickle.load(open(dump_dir + \"no_shuffling.pkl\", \"rb\"))\n",
    "permutations = permutations = [np.arange(60000)]*len(stats_no_shuffling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten the list for each epoch\n",
    "stats_no_shuffling_flat = []\n",
    "for stat in stats_no_shuffling:\n",
    "    stats_no_shuffling_flat.append(\n",
    "        [v for sublist in stat for v in sublist]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remap the indices based on the permutations list\n",
    "for stat, perm in zip(stats_no_shuffling_flat, permutations):\n",
    "    for i in range(len(stat)):\n",
    "        stat[i][0] = perm[i]\n",
    "        stat[i][1] = stat[i][1].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resort in increasing index order\n",
    "for i in range(len(stats_no_shuffling_flat)):\n",
    "    stats_no_shuffling_flat[i] = sorted(stats_no_shuffling_flat[i], key=lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get percentile splits for all 5 epochs\n",
    "num_quantiles = 10\n",
    "percentile_splits = bin_losses(stats_no_shuffling_flat, num_quantiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeYAAAD+CAYAAADvYaaNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAF1BJREFUeJzt3XuQpXV95/F3OxIQgw0MG8exdxwM7JeYbCISLJWbi9G4rmwqISs6EkVkHBTWEYyKxEuBlJiYkBrdIQiImMogEitSuiBJGWHQcFHwsiuRb4yyMu0wzKggjMjFofeP52npaXpOn27O75xf93m/qqa6+5znPL9v93y6v8/vuZ2RiYkJJElSHZ406AIkSdJjbMySJFXExixJUkVszJIkVcTGLElSRWzMkiRV5MmDLmC6iHgxcAXwr1Me3paZ/6MH674UuDwzr5nHaw8ArszM35rhuSOBezPz/3Sxnn2BfwO+3T702cxcN22ZpcAHM3NNRPw/4LzM/Ej73EHABZn54i7rPgP4PeBRYAI4MzNvjYjrgJMz8/Yu1rE/8FngW8B64BPA54DXAAcBbwO+lJlf7aamdp0jwKVtDT/v9nUl1Ji5iPgwcDjN7+iFmXnRtOfN3ALOHNSZu/a1ewI3AGdMf72560/uqmvMrS9l5qsHXcSkiPgTYC2w3y4WORG4HJg1rMDzgE9l5v/ssMw5NKGYdHpE/GNmZjf1ToqI5wD/HTgsMyci4rnAJ4Hfmct6gMOAf87Mt0fEe2h+WT4aEa8ByMwPzXF9tPVcBrwTOGuury+gmsxFxH8BDsjMF0bE7sBtEfGZzLxnymJmbo4qzBxUlLsp1tM0tpmYuzmaT+5qbcwzard8bqfZchkBjsvMLRHxVzSzC4DLMnNdRBwIXAz8CvAAMBn+NRHxTmAUeDNNwK5ov34K8M7MvG7a0PcARwHfm6GmQ4CXA8+LiH8FjqDZqnoI+C7wpsx8ZMpLDmmX3QhsBd6amXdNWd/TgEMz881TXnM68MmIOGza2AcDHwV2AA8CqzPzzimLbAVWACdGxDWZ+c2IeP6U598fEU8HnkqzRbiCZqvu1e36twDPB94D7BkR9wFvAB6OiPEpdVxK88u6DHgFsCfw68CfZ+al7Zjrgfvbmh7MzBOALwLnRcQHMvPR6T/bGgwoczcC32w/nwCWAL/MkJlb3JmDwf2ti4g/pZktj8xQk7nrU+5qPcZ8dERcN+XfO6Y8d0O7a+PTwJkR8Upgf+AFNIFdFRH/GfhL4NzMfCHwMeDg9vW3ZubRNP/JJ9D8UJcBxwCraH7QO8nM/52ZP5up0My8FbiGZmvoZzRbREdn5uHAvcCaaS+5HXh/Zh4FXNnWMdULgOlbi1cD/xd417THLwJObdd1PnDetNp+RLsVCdwYEbcDr5yyyFXtz+ILwB/v4vu7E/gQzR+Bs2h2yZyXmZ+daXlgNDNf2Y57RvvYBcAJ7Vi/3LjJzB004X3c4YEBqCZzmflgZt4TEbvRbPVfmJnbpzxv5na2UDMHFeUuIl4CHDj9sMkkc/c4xXJX64y50+6dL7UfbwD+ANgEfDkzJ4BHIuIm4DlA0Mw8yMwrACJiFXBr+/otwJ6ZeVtErAc+BewGfOQJ1P1s4LbMvL/9+nrgZTPU/0D7+WeBs6c9vx9w9wzrPh24hZ1n7cszc3JmdT1NqH4pmuPi92Xmie3XvwtcHRHXtotM/Vksm2HMx201d2Gynk3AHlPqvK39/Ms8tkUPcBewdB7j9FpVmYuIfYDPANdl5rkd6jZzCzdzUFfu3gg8q52tH0Qz290y5f97KnNXMHe1zpg7OaT9eBhwG/Ad2l077QzjRTS7Vb4DHNo+/tqImDzOsdOxk3aLc6/M/G/A63n8Vl03HqX5Wd4BPCcinto+fhTNyQ9TXQwc237+Eh4LzKStwN7TB2h/AdYAU0+e2BwRv91hrN8G/iYiJkPzb8BPaXYHweOPIz0IPAMgIp4F7Du9ji7MdGxqU3sMCJqt5Kn2ofmea9bXzEXEU4B/Bi7JzA/soiYz95jFmDnoc+4yc1VmHtbO0q+h2dU9vSmbu8cUy12tM+aj2622qf5r+/GEiDidZlfKn2TmjyPixRFxI80xlisy8+vtLqGPRXMA/wHgeB4L+lTfpTn+8DrgYeB986j3ZpotuOOA9wPXRsSjwL/z2C6OSWcAl0TEW9rv4aRpz98E/PlMg2TmdRHxKR7bVbUa+F/RnPX3C5ot3qnL/0NE/AZwc0Rsp/mFekdm/jQiZhriFuDeiLiZ5pf9jlm/8+68heZ73k7zM/4hQEQ8CRhj57NSB6WmzJ1MMyNZHRGr28fekJlT/z/MXGcLIXNQV+66Ye4660nuRhbSu0vFHE57X8gi4gLgY5n5jUHX0gsRcQrNH5FtEXEO8HBmnh0RrwCel5nnDLjEXTJzC9NCzhyYu4WqV7lbiLuyh8H7aLa8Fou7gX+KiC8DzwXWt1u+q4C/HmhlmmTmNAjmbgYLasYsSdJi54xZkqSKFDn5KyJeQHPgfxPNqf5jwEqaC9tPy8xtJcbV8DJzGgRzpxJKnZX9GpoLy78VEZcDSzPzpdHcanA18MHJBaO55eChNNd47ZhxbRoWS2guYfhaZj40x9eaOc2XuVO/dcxcqcb818D7IuInwK8BP2gfHweWT1v2UJoLsaVJRwBfmeNrzJyeKHOnfpsxc6Ua8wrgnMz8fkRcBTy9fXwM2Dxt2bsANmzYwLJlM92QRcNiy5YtvPa1r4U2E3Nk5jQv5k79NlvmSjXmceAvI+Je4DJgv4g4n+bOJydPW3YHwLJlyxgbGytUjhaY+ezmM3N6osyd+m3GzBVpzJn5feCPSqxbmomZ0yCYO5Xg5VKSJFXExixJUkVszJIkVcTGLElSRWzMkiRVxMYsSVJFbMySJFXExixJUkVszJIkVcTGLElSRUrdK1uS1K2Rke6XnZgoV4eq4IxZkqSK2JglSaqIjVmSpIrYmCVJqoiNWZKkitiYJUmqiI1ZkqSKFLmOOSLGgLOBe4AR4C5gJTAKnJaZ20qMq+Fl5jQI5k4llJoxHwQcDawAfgQcmZmnAB8HVhcaU8PNzGkQzJ16rtSdvzbRhPUO4J+A8fbxcWB5oTE13MycBsHcqedKzZhPBUYzcwL4KfCs9vExYHOhMTXczJwGwdyp50rNmD8BnB0RdwI3Aw9HxPnAPsDJhcbUcDNzGgRzp54r0pgz8+vAMSXWLc3EzGkQzJ1K8HIpSZIqYmOWJKkiNmZJkipiY5YkqSI2ZkmSKmJjliSpIjZmSZIqUuoGI5IkLU4jI90vOzEx59U7Y5YkqSI2ZkmSKmJjliSpIjZmSZIqYmOWJKkiNmZJkipiY5YkqSI2ZkmSKmJjliSpIjZmSZIqUuSWnBFxCnAosBtwOPBpYHdgFFiTmQ+VGFfDy8xpEMydSigyY87M9Zl5AjAO/CEwmplrgY3AsSXG1HAzcxoEc6cSiu3KjoiDaLYa96AJLe3H5aXG1HAzcxoEc6deK/nuUm8BPgw8DDyzfWwM2FxwTA03M6dBMHfqqZKN+YDM/B5ARPwkItbRbFWuLjimhpuZ0yCYO/VUscacma+Y8vmZpcaRJpk5DYK5U695uZQkSRWxMUuSVBEbsyRJFbExS5JUERuzJEkVsTFLklQRG7MkSRWxMUuSVBEbsyRJFbExS5JUERuzJEkVsTFLklQRG7MkSRWxMUuSVBEbsyRJFbExS5JUERuzJEkVsTFLklSRJ5dYaUSsBN4LbAXuB/YGdgdGgTWZ+VCJcTW8zJwGwdyphFIz5rcD48AzgLuB0cxcC2wEji00poabmdMgmDv1XKnGfABwJbAaOJ4muLQflxcaU8PNzGkQzN1iMDLS/b8+KNWYtwD3ZeYj7dfPbD+OAZsLjanhZuY0COZOPVfkGDPwF8C5EbEVuBj4zYhYR3PcZXWhMTXczJwGYTC5m8vMbWKiWBkqo0hjzszvAK8qsW5pJmZOg2DuVIKXS0mSVBEbsyRJFZl1V3ZEvB84FXgEGAEmMtOzDbU4eexO0oB1c4z5lcCKzPx56WIkSRp23ezK3kozW5YkSYXtcsYcEZ8CJoCnA9+IiG+3T01k5qp+FCdJ0rDptCv7gr5VIUmSgA67sjNzY2ZuBJ4GvKT9/N3AHv0qTpKkYdPNyV9nAS9vPz8O+ALwj8UqkiRpiHVz8tcjmbkVIDN/CuwoW5IkScOrmxnzVyPiMuBG4FDgG2VLkiRpeHXTmN8K/AEQwGcy83NlS5IkaXh1ulxqCbAEuBx4NXA1sCQivpSZR/epPkmShkqnGfOJwJnAMuB2mttx7gC+0oe6JEkaSrtszJl5EXBRRJyYmZf0sSZJkoZWN8eYr4+IdwO70cyal2fmmrJlSZI0nLq5XOpv24+HA/sDS8uVI0nScOumMT+QmecC45l5As29syVJUgHd7MoeiYhlwK9GxFOBfWd7QUSsAD4HfBO4i+bs7t2BUWBNZj40/5KlxzNzGgRzpxK6mTGfBfwh8HfAHTS35JzNUcDd7edbgNHMXAtsBI6dR53SbMycBsHcqedmnTFn5vXA9e2Xv9bler8KfJEmsF8Erm0fHwd+Z441St0wcxoEc6ee63SDkTto3o/5cTLz2bOs92Dgxsx8NCJGgMnlx4DN8ylUmoWZ0yCYO/Vcpxnz54FDaLYC/w64cw7r/S7w4YjYBnwaWBER62iOu6yeZ61SJ2ZOg2Du1HOdbjDy1oh4EvAy4L00J31dCVwBdDyhITNvBV7VwzqljsycBsHcqYSOx5gz81HgGuCaiNgX+Bvgo8BT+lCbJElDp2NjbmfMLwVeAzyX5ozsQ/tQlyRJQ6nTyV/raS4FuA64MDNv6FdRkiQNq04z5jcDP6a5Fu/YiJiguVf2RGYu70dxkiQNm04nf3Vz8xFJUu1GRrpfdmLGq2TVRzZfSZIqYmOWJKkiszbmiPjdaV8fVa4cSZKGW6ezso8AngOcFhHntQ8vAU4BfqsPtUmSNHQ6nZV9D7CM5i3MntE+9ijwztJFSZI0rDqdlf1t4NsRcVFmejN2SZL6YNa3fQR+LyLeTTNznryOebZ3l5IkSfPQTWN+F3AMsKlwLZIkDb1uGvP3M/Pfi1ciSZK6aswPRMQXgG8CEwCZeWbRqiRJmq8Ffqezbhrz1cWrkCRJQHd3/toA7AY8G/gBcFXRiiRpLkZGuv8nLQDdNOYLgBXAy4C9gL8tWpEkSUOsm13Zv56ZJ0XEEZn5+Yg4o5sVR8QG4PPAfwRWAqPAaZm5bd7VSrMwd+o3M6de62bG/OSI2A+YiIi9aO7+1VFEnA5sb788MjNPAT4OrJ53pdIszJ36zcyphG5mzO8B/oXmtpw3AW/rtHBEHAPcC9xI0/i3tk+NA8vnXanUgblTv5m5PlrgZ1nP1ayNOTM3RsRLgZ8DKzPza7O85Hia+2xH+/Xk1uQY4K09VYq5U7+ZORUxa2OOiAuA8cw8JyLeExHHZ+baXS2fmce1rzsBeBB4ekScD+wDnNybsqWdmTv126LO3JDNUGvTza7sgzPzZIDMXBsR13ez4sy89IkUJs2HuVO/mTn1Wjcnf41ExFKAiNib7pq5JEmah26a7FnALRHxE2Bv4C1lS5IkaXh105j3Bg4A9gO2ZqYHFCRJ8+cx7I66acxvyswNwN2li5Ekadh105h3j4hvAEl7c5HMXFW0KkmShlQ3jfldxauQJElAd2dlfx14KfA6YCnww6IVSZIWFt/hq6e6acyXAN8H/hOwheY+sJIkqYBuGvPSzLwEeCQzbwDc5JEkqZBuGjMRcVD7cQzYUbQiSZKGWDcnf60FPgH8BvAZvMGIJEnFdGzMEfE04HuZ+cI+1SNJ0lDb5a7siDgV+BbwrYj4/f6VJEnS8Op0jHkVzfuMvhB4W3/KkSRpuHVqzA9m5sOZ+SPgV/pVkCRJw6yrs7LxEilJkvqi08lfvxkRl9E05cnPAe+VLUlSKZ0a86umfH5B6UIkSVKHxpyZG/tZiCRJ6u4GI3MWEQcCH6K5t/bXgP8ArARGgdMyc1uJcTW8zJwGwdyphG5P/pqrUeBPaS6zWgUcmZmn0LwBxupCY2rQBvsOM2ZOg2Du1HNFGnNm3gI8DFwFXAdsbZ8aB5Z3vSLfSkxd6lnmpDkwdyqhSGOOiOfSXAf9MuAQYL/2qTFgc4kxNdzMnAbB3KmEIseYaW5I8rGI+DHNezmPR8T5wD7AyYXG1HAzcxoEc6eeK9KYM/OrwB+XWLc0EzOnQTB3KqHUyV+SJGkebMySJFXExixJUkVszJIkVcTGLElSRWzMkiRVxMYsSVJFbMySJFXExixJUkVszJIkVcTGLElSRWzMkiRVxMYsSVJFbMySJFXExixJUkVszJIkVcTGLElSRWzMkiRV5MklVhoRLwLWAtuBHwC/CuwOjAJrMvOhEuNqeJk5DYK5UwmlZsz7ACdl5huBw4HRzFwLbASOLTSmhpuZ0yCYO/VckcacmVcB2yPiz4B/Acbbp8aB5SXG1HAzcxoEc6cSijTmiNgLuBi4CbgEeGb71BiwucSYGm5mToNg7lRCkWPMwDrgQOANwA7ghxGxjua4y+pCY2q4mTkNgrlTzxVpzJl5Yon1Srti5jQI5k4leLmUJEkVsTFLklQRG7MkSRWxMUuSVBEbsyRJFbExS5JUERuzJEkVsTFLklQRG7MkSRWxMUuSVBEbsyRJFbExS5JUkVLvLiUNh5GR7pedmChXh6RFwxmzJEkVsTFLklQRG7MkSRWxMUuSVBEbsyRJFSl2VnZEHAD8fWYeHBHvAFYCo8Bpmbmt1LgabuZO/Wbm1GtFZswRsQw4CfhZROwBHJmZpwAfB1aXGFMyd+o3M6cSijTmzNySmWcA24F9ga3tU+PA8hJjSuZO/WbmVEI/jjFvBZa2n48Bm/swpmTu1G9mTj1RvDFn5i+AayPifOBNwPrSY0rmTv1m5tQrRW/JmZkvbz+uKzmONJW5W+AW4G1OzZx6yculJEmqiI1ZkqSK2JglSaqIjVmSpIrYmCVJqoiNWZKkitiYJUmqSNHrmCVNswCv0ZXUX86YJUmqiI1ZkqSKuCtbUlnd7r53170EOGOWJKkqNmZJkiqyuHZle8arFpu5ZrofvwPumpaKWlyNWdLc2WilqrgrW5Kkijhj1q55aECS+s4ZsyRJFenLjDkingn8FfAT4LbMXN+PcWc1bDPCIft+q82dFi0zp17o167sNcBHMvOGiLg6Ii7MzEfa55YAbNmyZYbq5lDe+Hj55fffv/vl77ijvuVr+3lOMyUDS7pfUUe7yt3CyVw//g+6fU1ty0++pq7cLY6/dS7f2+WnmS1zIxN9mBlFxIXABzJzU0RsANZm5o/a5w4Hvly8CC0kR2TmV57oSnaVOzOnXXjCufNvneZoxsz1a8Z8JzAGbAL2Be6d8tzXgCOAu4AdfapHdVoCPIMmE72wq9yZOU3Vy9z5t07d6Ji5fs2YlwHnAfcDt2TmRcUH1dAzd+o3M6de6EtjliRJ3Vm01zFHxIuAtcB24AeZeXahcTYAn8/My3u83pXAe4GtwP2Z+cFerr8dYww4G7gHGMnM03u47gOAv8/MgyPiHcBKYBQ4LTO39Wqc2pi7Wddv5npsoWeuXfdKzN0vLebrmPcBTsrMNwKHlxggIk6n+WUo4e3AOM1xiBsLjXEQcDSwoh2rJ9rdeScBP4uIPYAjM/MU4OPA6l6NUylz15mZ672FnjkwdztZtI05M68CtkfEnwEber3+iDiG5sSOUk3zAOBKmv/c9xUaYxNNWF8F/H5E7NmLlWbmlsw8g+YXeV+arWBofiGW92KMWpm7WZm5HlsEmQNzt5NF25gjYi/gYuCmzPxkgSGOB54PvB54Y0Qs7fH6twD3tddA3t/jdU86FRjNzIl2jBKHNrYCkz+bMWBzgTGqYe5mZeZ6bBFkDszdThbtMWZgHXAg8IaIeF1mvr6XK8/M4wAi4gTgwcz8cS/XD/wFcG5EbAU+3eN1T/oEcHZE3AncnJn39XqAzPxFRFwbEefT7HI7uddjVMbcdWbmem+hZw7M3U48K1uSpIos2l3ZkiQtRDZmSZIqYmOWJKkiNmZJkipiY5YkqSI2ZkmSKmJjliSpIv8f9TvyYtYy2cEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 576x288 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fr = [0, 1, 3]\n",
    "to = [0, 0, 0]\n",
    "names = ['1 to 5', '2 to 5', '4 to 5']\n",
    "\n",
    "all_matches = []\n",
    "for f, t in zip(fr, to):\n",
    "    percent_matches = []\n",
    "    for i in range(num_quantiles):\n",
    "        percentile_all = []\n",
    "        for j in range(f, len(percentile_splits)-t):\n",
    "            percentile_all.append(percentile_splits[j][i])\n",
    "        matching = reduce(np.intersect1d, percentile_all)\n",
    "        percent = 100 * len(matching) / len(percentile_all[0])\n",
    "        percent_matches.append(percent)\n",
    "    all_matches.append(percent_matches)\n",
    "    \n",
    "        \n",
    "fig, axes = plt.subplots(1, 3, figsize=(8, 4))\n",
    "for i, (ax, match, n) in enumerate(zip(axes, all_matches, names)):\n",
    "    ax.bar(range(1, num_quantiles+1), match, width=0.9, color='r')\n",
    "    if i == 0:\n",
    "        ax.set_ylabel('Percent Match')\n",
    "    ax.set_title('Epochs {} (No Shuffling)'.format(n))\n",
    "    ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    ax.set_ylim([0, 90])\n",
    "    \n",
    "plt.savefig(plot_dir + \"no_shuffling.jpg\", format=\"jpg\", dpi=1000, bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
