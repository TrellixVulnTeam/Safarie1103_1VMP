{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "# Final Project\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Milestone 1 : Data Source\n",
    "#### https://www.kaggle.com/c/zillow-prize-1\n",
    "\n",
    "#### Description\n",
    "There are two data sets with over 1 million records each and 58 columns. properties_2016 and properties_2017 datasets contain data for each year. The data we will use for this project will be a small sample of the master data. \n",
    "\n",
    "The two datasets are linked by parcleid.\n",
    "\n",
    "I transactions dataset, the trabsaction date shows the date the property was sold and logerror is the log10( estimated price - price sold).\n",
    "\n",
    "Properties dataset has the physical information about the properities. The columns on the properties dataset will have to be renamed. Subsets of data can be used to group by region, and other features such as number of bedrooms, square footage, etc.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load Libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import xlrd\n",
    "import numpy as np\n",
    "# Load Data\n",
    "transactions_2016 = \"Data/transactions_2016.json\"\n",
    "transactions_2017 = \"Data/transactions_2017.json\"\n",
    "\n",
    "properties_2016  =  \"Data/properties_2016.csv\"\n",
    "properties_2017  =  \"Data/properties_2017.csv\"\n",
    "data_dictionary = \"Data/data_dictionary.xlsx\"\n",
    "\n",
    "transactions_2016 = pd.read_json(transactions_2016)\n",
    "transactions_2017 = pd.read_json(transactions_2017)\n",
    "properties_2016 = pd.read_csv(properties_2016)\n",
    "properties_2017 = pd.read_csv(properties_2017)\n",
    "data_dictionary = pd.read_excel(data_dictionary)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "transactions_2016.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "properties_2016.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "properties_2016.columns"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_dictionary.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Milestone 2 : Cleaning/formatting flat file sources\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will first combine the properties_2016 and properties_2017 and calle the result properties. We will also combine the two transactions datasets.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "properties = pd.concat([properties_2016,properties_2017],axis=0)\n",
    "print(properties_2016.shape)\n",
    "print(properties_2017.shape)\n",
    "print(properties.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "transactions = pd.concat([transactions_2016,transactions_2017],axis=0)\n",
    "print(properties_2016.shape)\n",
    "print(properties_2017.shape)\n",
    "print(properties.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "properties.columns"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Get rid of the Unamed column."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "properties = properties.loc[:, ~properties.columns.str.contains('^Unnamed')]\n",
    "properties.columns"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Rename column names in properties dataset."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "properties = properties.rename(columns=\n",
    "                        {\n",
    "  'parcelid':'parcelid', \n",
    "  'yearbuilt':'build_year', \n",
    "  'basementsqft':'area_basement', \n",
    "  'yardbuildingsqft17':'area_patio', \n",
    "  'yardbuildingsqft26':'area_shed',  \n",
    "  'poolsizesum':'area_pool', \n",
    "  'lotsizesquarefeet':'area_lot', \n",
    "  'garagetotalsqft':'area_garage', \n",
    "  'finishedfloor1squarefeet':'area_firstfloor_finished', \n",
    "  'calculatedfinishedsquarefeet':'area_total_calc', \n",
    "  'finishedsquarefeet6':'area_base', \n",
    "  'finishedsquarefeet12':'area_live_finished', \n",
    "  'finishedsquarefeet13':'area_liveperi_finished', \n",
    "  'finishedsquarefeet15':'area_total_finished',  \n",
    "  'finishedsquarefeet50':'area_unknown', \n",
    "  'unitcnt': 'num_unit', \n",
    "  'numberofstories': 'num_story',  \n",
    "  'roomcnt':'num_room', \n",
    "  'bathroomcnt':'num_bathroom', \n",
    "  'bedroomcnt':'num_bedroom', \n",
    "  'calculatedbathnbr':'num_bathroom_calc', \n",
    "  'fullbathcnt':'num_bath', \n",
    "  'threequarterbathnbr':'num_75_bath',  \n",
    "  'fireplacecnt':'num_fireplace', \n",
    "  'poolcnt': 'num_pool',  \n",
    "  'garagecarcnt':'num_garage', \n",
    "  'regionidcounty':'region_county', \n",
    "  'regionidcity':'region_city', \n",
    "  'regionidzip':'region_zip', \n",
    "  'regionidneighborhood':'region_neighbor', \n",
    "  'taxvaluedollarcnt':'tax_total', \n",
    "  'structuretaxvaluedollarcnt':'tax_building', \n",
    "  'landtaxvaluedollarcnt':'tax_land', \n",
    "  'taxamount':'tax_property', \n",
    "  'assessmentyear':'tax_year', \n",
    "  'taxdelinquencyflag':'tax_delinquency', \n",
    "  'taxdelinquencyyear':'tax_delinquency_year', \n",
    "  'propertyzoningdesc':'zoning_property', \n",
    "  'propertylandusetypeid':'zoning_landuse', \n",
    "  'propertycountylandusecode':'zoning_landuse_county', \n",
    "  'fireplaceflag':'flag_fireplace', \n",
    "  'hashottuborspa':'flag_tub', \n",
    "  'buildingqualitytypeid':'quality', \n",
    "  'buildingclasstypeid':'framing', \n",
    "  'typeconstructiontypeid':'material', \n",
    "  'decktypeid':'deck', \n",
    "  'storytypeid':'story', \n",
    "  'heatingorsystemtypeid':'heating', \n",
    "  'airconditioningtypeid':'aircon', \n",
    "  'architecturalstyletypeid':'architectural_style' \n",
    "})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "properties.columns"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Check new column names\n",
    "properties[['num_bedroom','num_bathroom']]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Rename column names in transactions dataset."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "transactions = transactions.rename(columns={'parcelid':'parcelid','date':'transactiondate'})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "transactions.columns"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Check out the new columns"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "transactions[['parcelid','transactiondate']]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "propertiesAndTransactions = pd.merge(properties,transactions,on='parcelid')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "check out the merge"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "propertiesAndTransactions[['parcelid','num_bedroom','transactiondate','logerror']].head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "let's take care of missings"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "column_names = propertiesAndTransactions.columns\n",
    "print('sum\\n', propertiesAndTransactions.isnull()[column_names].sum())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('mean\\n', propertiesAndTransactions.isnull()[column_names].mean())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's look at columns woth more than 80% missing values"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "propertiesAndTransactions.isnull()[column_names].sum()\n",
    "# this shows columns and the number of NaN's.Note parcelID has no missing values."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Make a list of columns with moe than 80% missing data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "remove_columns = propertiesAndTransactions.columns[propertiesAndTransactions.isnull().mean() > .8]\n",
    "print(remove_columns)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Drop the columns"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "propertiesAndTransactions = propertiesAndTransactions.drop(columns = remove_columns)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Check results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(len(propertiesAndTransactions.columns))\n",
    "print(propertiesAndTransactions.columns)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Check results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(len(propertiesAndTransactions.columns))\n",
    "print(propertiesAndTransactions.columns)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's check the missing values mean"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('mean\\n', propertiesAndTransactions.isnull()[propertiesAndTransactions.columns].mean())\n",
    "# we see the means to all be below 80%."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Are there any duplicate?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "propertiesAndTransactions[propertiesAndTransactions.duplicated(keep=False)]\n",
    "# There are no duplocate rows; however, there are duplicate parcelIDs and corresponding latitude and Longitude."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "propertiesAndTransactions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The two datasets have been merged, columns with more than 80% missing values were removed. The final dataset 'propertiesAndTransactions' will be used in the next milestone."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Milestone 3 : Webscaraping Data Source\n",
    "#### Description\n",
    "Using webscraping techniques, we will use 'latitude', 'longitude' from properties dataset to access properties and get current data for those locations. The property description of homes in given region will be stored into a dataset with as many features as in properties dataset we can grab. This dataset can then be used to do some price comparision between properties in 2016 and 2017. Getting data from years prior(say 10 years), we will be able to create trend charts and see market fluctuations."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Build a table consisiting of the parcelID, latitude and longitude of the properties.\n",
    "# This table will be used to get data from www.trulia.com by web scraping\n",
    "\n",
    "LonLat = pd.DataFrame(propertiesAndTransactions[['parcelid','latitude','longitude']])\n",
    "LonLat\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# We will remove duplicate parcelIDs here since we are only interested in comparable values near each parcelID.\n",
    "LonLat = LonLat.sort_values('parcelid', ascending=False)\n",
    "LonLat = LonLat.drop_duplicates()\n",
    "LonLat.reset_index(drop=True)\n",
    "LonLat"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('sum\\n', LonLat.isnull()[['parcelid','latitude','longitude']].sum())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# This dictionary is used to return state code. trulia requires the state code rather than state name\n",
    "us_state_abbrev = {\n",
    "    'Alabama': 'AL',\n",
    "    'Alaska': 'AK',\n",
    "    'American Samoa': 'AS',\n",
    "    'Arizona': 'AZ',\n",
    "    'Arkansas': 'AR',\n",
    "    'California': 'CA',\n",
    "    'Colorado': 'CO',\n",
    "    'Connecticut': 'CT',\n",
    "    'Delaware': 'DE',\n",
    "    'District of Columbia': 'DC',\n",
    "    'Florida': 'FL',\n",
    "    'Georgia': 'GA',\n",
    "    'Guam': 'GU',\n",
    "    'Hawaii': 'HI',\n",
    "    'Idaho': 'ID',\n",
    "    'Illinois': 'IL',\n",
    "    'Indiana': 'IN',\n",
    "    'Iowa': 'IA',\n",
    "    'Kansas': 'KS',\n",
    "    'Kentucky': 'KY',\n",
    "    'Louisiana': 'LA',\n",
    "    'Maine': 'ME',\n",
    "    'Maryland': 'MD',\n",
    "    'Massachusetts': 'MA',\n",
    "    'Michigan': 'MI',\n",
    "    'Minnesota': 'MN',\n",
    "    'Mississippi': 'MS',\n",
    "    'Missouri': 'MO',\n",
    "    'Montana': 'MT',\n",
    "    'Nebraska': 'NE',\n",
    "    'Nevada': 'NV',\n",
    "    'New Hampshire': 'NH',\n",
    "    'New Jersey': 'NJ',\n",
    "    'New Mexico': 'NM',\n",
    "    'New York': 'NY',\n",
    "    'North Carolina': 'NC',\n",
    "    'North Dakota': 'ND',\n",
    "    'Northern Mariana Islands':'MP',\n",
    "    'Ohio': 'OH',\n",
    "    'Oklahoma': 'OK',\n",
    "    'Oregon': 'OR',\n",
    "    'Pennsylvania': 'PA',\n",
    "    'Puerto Rico': 'PR',\n",
    "    'Rhode Island': 'RI',\n",
    "    'South Carolina': 'SC',\n",
    "    'South Dakota': 'SD',\n",
    "    'Tennessee': 'TN',\n",
    "    'Texas': 'TX',\n",
    "    'Utah': 'UT',\n",
    "    'Vermont': 'VT',\n",
    "    'Virgin Islands': 'VI',\n",
    "    'Virginia': 'VA',\n",
    "    'Washington': 'WA',\n",
    "    'West Virginia': 'WV',\n",
    "    'Wisconsin': 'WI',\n",
    "    'Wyoming': 'WY'\n",
    "}\n",
    "\n",
    "abbrev_us_state = dict(map(reversed, us_state_abbrev.items()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import urllib.parse\n",
    "import urllib.error\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import Request, urlopen\n",
    "import geopy\n",
    "from geopy.geocoders import Nominatim\n",
    "\n",
    "def create_url(city,state,zipcode):\n",
    "    # Creating trulia URL based on the filter.\n",
    "\n",
    "    url = \"https://www.trulia.com/\" + state + \"/\" + city + \"/\" + zipcode\n",
    "    return url\n",
    "\n",
    "def get_response(url):\n",
    "    ret = None\n",
    "    try:\n",
    "        for i in range(5):\n",
    "            response = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "            print(\"status code received:\", response.status_code)\n",
    "            if (response.status_code != 200):\n",
    "                return None\n",
    "            else:\n",
    "                return response\n",
    "    except:\n",
    "        print('exception in get_response')\n",
    "        return None\n",
    "\n",
    "def GetCityStateZip(lat,lon):\n",
    "    lat = lat/10**6\n",
    "    lon = lon/10**6\n",
    "    geolocator = Nominatim(timeout=5)\n",
    "    #print(location.raw)\n",
    "    try:\n",
    "        location = geolocator.reverse((lat, lon))\n",
    "        city = location.raw['address']['city']\n",
    "        state = us_state_abbrev[location.raw['address']['state']]\n",
    "        zipcode = location.raw['address']['postcode'].split('-')[0]\n",
    "    except:\n",
    "        city = \"\"\n",
    "        state = \"\"\n",
    "        zipcode = \"\"\n",
    "\n",
    "    return city,state,zipcode"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "def GetComp(parcelId,latitude,longitude):\n",
    "    city,state,zipcode = GetCityStateZip(latitude,longitude)\n",
    "    #print(parcelId,latitude,longitude)\n",
    "    #print(\"city=\", city)\n",
    "    #print(\"state=\", state)\n",
    "    #print(\"zipcode=\",zipcode)\n",
    "\n",
    "    emptylistings_json = {}\n",
    "    emptylistings_json['parcelId'] = {0:parcelId}\n",
    "    emptylistings_json['price'] = {0:np.nan}\n",
    "    emptylistings_json['bedrooms'] = {0:np.nan}\n",
    "    emptylistings_json['bathrooms'] = {0:np.nan}\n",
    "    emptylistings_json['floorSpace'] = {0:np.nan}\n",
    "    emptylistings_json['region'] = {0:np.nan}\n",
    "\n",
    "    if (city == \"\" or state == \"\" or state == \"\"):\n",
    "        return(pd.DataFrame(emptylistings_json))\n",
    "\n",
    "    url = create_url(city,state,zipcode)\n",
    "\n",
    "    #req = Requests(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "    #webpage = urlopen(req).read()\n",
    "    #soup = BeautifulSoup(webpage, 'html.parser')\n",
    "\n",
    "    response = get_response(url)\n",
    "    #print(response.text)\n",
    "    if not response:\n",
    "        print(\"Failed to fetch the page, please check `response.html` to see the response received from zillow.com.\")\n",
    "        return(pd.DataFrame(emptylistings_json))\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    html = soup.prettify('utf-8')\n",
    "\n",
    "    details = {}\n",
    "    parcels = {}\n",
    "    listings_json = {}\n",
    "    index = 0\n",
    "\n",
    "    for price in  soup.findAll('div',attrs={'data-testid': 'property-price'}):\n",
    "        details.update({index:price.text.strip()})\n",
    "        parcels.update({index:parcelId})\n",
    "        index = index + 1\n",
    "\n",
    "    listings_json['parcelId'] = {}\n",
    "    listings_json['parcelId']  = parcels\n",
    "    listings_json['price'] = {}\n",
    "    listings_json['price']  = details\n",
    "    #print(listings_json['price'])\n",
    "\n",
    "\n",
    "\n",
    "    details = {}\n",
    "    index = 0\n",
    "    for bedroom  in  soup.findAll('div',attrs={'data-testid': 'property-beds'}):\n",
    "        details.update({index:bedroom.text.strip()})\n",
    "        index = index + 1\n",
    "\n",
    "    listings_json['bedrooms'] = {}\n",
    "    listings_json['bedrooms']  = details\n",
    "    #print(listings_json)\n",
    "\n",
    "\n",
    "\n",
    "    details = {}\n",
    "    index = 0\n",
    "    for bathroom  in  soup.findAll('div',attrs={'data-testid': 'property-baths'}):\n",
    "        details.update({index:bathroom.text.strip()})\n",
    "        index = index + 1\n",
    "\n",
    "    listings_json['bathrooms'] = {}\n",
    "    listings_json['bathrooms']  = details\n",
    "    #print(listings_json)\n",
    "\n",
    "\n",
    "\n",
    "    details = {}\n",
    "    index = 0\n",
    "    for floorSpace  in  soup.findAll('div',attrs={'data-testid': 'property-floorSpace'}):\n",
    "        details.update({index:floorSpace.text.strip()})\n",
    "        index = index + 1\n",
    "\n",
    "    listings_json['floorSpace'] = {}\n",
    "    listings_json['floorSpace']  = details\n",
    "    #print(listings_json)\n",
    "\n",
    "\n",
    "\n",
    "    details = {}\n",
    "    index = 0\n",
    "    for region  in  soup.findAll('div',attrs={'data-testid': 'property-region'}):\n",
    "        details.update({index:region.text.strip()})\n",
    "        index = index + 1\n",
    "\n",
    "    listings_json['region'] = {}\n",
    "    listings_json['region']  = details\n",
    "    #print(listings_json)\n",
    "\n",
    "    #listings_table = pd.DataFrame()\n",
    "\n",
    "    #with open('house_details.json', 'w') as outfile:\n",
    "    #    json.dump(listings_json, outfile, indent=4)\n",
    "    #listings_table = pd.read_json(\"house_details.json\")\n",
    "    return pd.DataFrame(listings_json)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "LonLat[:5]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Here we get 20 compare properties for the parcelIDs. Note that a parcelID from propertiesAndTransactions table may have one ore more comps near it's latitude and longitude. This process sometime times out. We have taken care to continue collecting even after such exceptions."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "comp_listing_table = pd.DataFrame(columns={'parcelid','price','bedrooms','bathrooms','floorSpace','region'})\n",
    "\n",
    "dfs = []\n",
    "for index, row in LonLat[:20].iterrows():\n",
    "    parcelId = row['parcelid']\n",
    "    latitude = row['latitude']\n",
    "    longitude = row['longitude']\n",
    "    #print(parcelId,latitude,longitude)\n",
    "    Temp_listing_table = GetComp(parcelId,latitude,longitude)\n",
    "    #print(Temp_listing_table.shape)\n",
    "    dfs.append(Temp_listing_table)\n",
    "    #print(Temp_listing_table)\n",
    "\n",
    "\n",
    "comp_listing_table = pd.concat(dfs, ignore_index=True)\n",
    "print(comp_listing_table.shape)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(comp_listing_table)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "comp_listing_table.isnull()[comp_listing_table.columns].sum()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "comp_listing_table = comp_listing_table.dropna()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "comp_listing_table.isnull()[comp_listing_table.columns].sum()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "comp_listing_table.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Write scraped data to a file for safe keeps and also to avoid rescraping during development\n",
    "comp_listing_table.to_csv(\"data/comp_listing_table.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Read\n",
    "comp_listing_table = pd.read_csv(\"data/comp_listing_table.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "comp_listing_table"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### prepare the dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "comp_listing_table = comp_listing_table.loc[:, ~comp_listing_table.columns.str.contains('^Unnamed')]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "comp_listing_table['price']= comp_listing_table['price'].replace('[\\$,]', '', regex=True).astype(float)\n",
    "comp_listing_table"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "comp_listing_table['bedrooms']= comp_listing_table['bedrooms'].replace('bd', '', regex=True).astype(int)\n",
    "comp_listing_table"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "comp_listing_table['bathrooms']= comp_listing_table['bathrooms'].replace('ba', '', regex=True).astype(float)\n",
    "comp_listing_table"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "comp_listing_table['floorSpace'] = comp_listing_table['floorSpace'].replace('sqft', '', regex=True).replace(',','',regex=True).astype(np.int64)\n",
    "comp_listing_table.columns"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# now that we have our comp table built let's do some comparisons\n",
    "## We'll grab a property from propertiesAndTransactions and query the comp table.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# THis table has duplicates and NaNs removed so it is a subset of the propertiesAndTransactions table.\n",
    "LonLat"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "propertiesAndTransactions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Notice the duplicates\n",
    "selected_parcelid = propertiesAndTransactions['parcelid'] == 17294231\n",
    "propertiesAndTransactions[selected_parcelid]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "selected_parcelid = comp_listing_table['parcelId'] == 17294231\n",
    "comp_listing_table[selected_parcelid]\n",
    "\n",
    "## Milestone 4 data from API\n",
    "#### Description\n",
    "\n",
    "Googlemap API and matplotlib or equivalant will be used to locate properties by zipcode and display them on the map of the Unites States. We will convert 'longitude' and 'latitude' columns in properties dataset to zip code and use the zipcode in the API call.We will show the density of homes sold in various regions in the dataset. We will also show the properties we extracted using webscraping techniques."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "propertiesAndTransactions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Notice the duplicates\n",
    "selected_parcelid = propertiesAndTransactions['parcelid'] == 17294231\n",
    "propertiesAndTransactions[selected_parcelid]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "selected_parcelid = comp_listing_table['parcelId'] == 17294231\n",
    "comp_listing_table[selected_parcelid]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## data from API\n",
    "#### Description\n",
    "\n",
    "Googlemap API and matplotlib or equivalant will be used to locate properties by zipcode and display them on the map of the Unites States. We will convert 'longitude' and 'latitude' columns in properties dataset to zip code and use the zipcode in the API call.We will show the density of homes sold in various regions in the dataset. We will also show the properties we extracted using webscraping techniques."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# This is a sample code and does not pertain to this project. We will try to implement a function s\n",
    "import googlemaps\n",
    "from datetime import datetime\n",
    "\n",
    "gmaps = googlemaps.Client(key='AIzaSyDBRpHoEPCnVtcFPit-jVx26fkbrAemzN0')\n",
    "\n",
    "# Geocoding an address\n",
    "geocode_result = gmaps.geocode('1600 Amphitheatre Parkway, Mountain View, CA')\n",
    "\n",
    "\n",
    "print(geocode_result)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Look up an address with reverse geocoding\n",
    "reverse_geocode_result = gmaps.reverse_geocode((40.714224, -73.961452))\n",
    "\n",
    "reverse_geocode_result"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Request directions via public transit\n",
    "now = datetime.now()\n",
    "directions_result = gmaps.directions(\"Sydney Town Hall\",\n",
    "                                     \"Parramatta, NSW\",\n",
    "                                     mode=\"transit\",\n",
    "                                     departure_time=now)\n",
    "\n",
    "directions_result"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}