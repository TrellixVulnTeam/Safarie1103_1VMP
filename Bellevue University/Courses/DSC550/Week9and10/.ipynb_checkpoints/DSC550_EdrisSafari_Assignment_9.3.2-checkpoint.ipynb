{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this assignment,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np, json, re, pickle\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# convert to lower case\n",
    "def to_lower(string: str) -> str:\n",
    "    return string.lower()\n",
    "\n",
    "# Load libraries\n",
    "import unicodedata\n",
    "import sys\n",
    "\n",
    "def remove_puncs(text):\n",
    "\n",
    "    # Create a dictionary of punctuation characters\n",
    "    #punctuation = dict.fromkeys(i for i in range(sys.maxunicode)\n",
    "                            #if unicodedata.category(chr(i)).startswith('P'))\n",
    "\n",
    "    #text = text.translate(punctuation)\n",
    "    text=re.sub('</?.*?>',' <>', text)\n",
    "    text=re.sub('\\\\d|\\\\W+|_',' ',text)\n",
    "    text=re.sub('[^a-zA-Z]',\" \", text)\n",
    "\n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "file ='controversial-comments.jsonl'\n",
    "# possible orient value: split, records, index, columns, and values)\n",
    "# The following file is a subset of above file with the 1st 233537 lines.\n",
    "comments = pd.read_json(\"controversial-comments_small.jsonl\",orient=\"columns\",lines=True)\n",
    "comments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print('Size: ', len(comments), '\\n',\n",
    "      'Shape: ', comments.info(), '\\n',\n",
    "      'Categories: ', comments.con.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# since the size is humongus, I will take sample of the 2 categories.\n",
    "# by trial, sample of 50000 from each category can be easily handled by my machine\n",
    "\n",
    "size = 50000    # sample size\n",
    "replace = True  # with replacement\n",
    "fn = lambda obj: obj.loc[np.random.choice(obj.index, size, replace),:]\n",
    "\n",
    "controversy = comments.groupby('con', as_index=False).apply(fn)\n",
    "del comments\n",
    "controversy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "controversy['txt'] = controversy['txt'].apply(lambda x:to_lower(x))\n",
    "controversy['txt'] = controversy['txt'].apply(lambda x:remove_puncs(x))\n",
    "\n",
    "controversy.reset_index(drop=True, inplace=True)\n",
    "\n",
    "controversy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "controversy['txt'] = [word_tokenize(string) for string in controversy['txt']]\n",
    "controversy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Load stop words\n",
    "stop_words = stopwords.words('english')\n",
    "controversy['txt'] = controversy['txt'].apply(lambda x: [item for item in x if item not in stop_words])\n",
    "controversy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "controversy.to_pickle('./Data/pkld_controversy.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(0)\n",
    "\n",
    "# Set the number of features we want\n",
    "number_of_features = 10000\n",
    "\n",
    "# Load data and target vector from IMDB movie data\n",
    "(data_train, target_train), (data_test, target_test) = imdb.load_data(\n",
    "                num_words=number_of_features)\n",
    "\n",
    "# Convert IMDB data to a one-hot encoded feature matrix\n",
    "tokenizer = Tokenizer(num_words=number_of_features)\n",
    "features_train = tokenizer.sequences_to_matrix(data_train, mode=\"binary\")\n",
    "features_test = tokenizer.sequences_to_matrix(data_test, mode=\"binary\")\n",
    "\n",
    "# Start neural network\n",
    "network = models.Sequential()\n",
    "\n",
    "# Add fully connected layer with a ReLU activation function\n",
    "network.add(layers.Dense(units=16,\n",
    "                         activation=\"relu\",\n",
    "                         input_shape=(number_of_features,)))\n",
    "\n",
    "# Add fully connected layer with a ReLU activation function\n",
    "network.add(layers.Dense(units=16, activation=\"relu\"))\n",
    "\n",
    "# Add fully connected layer with a sigmoid activation function\n",
    "network.add(layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "# Compile neural network\n",
    "network.compile(loss=\"binary_crossentropy\", # Cross-entropy\n",
    "                optimizer=\"rmsprop\", # Root Mean Square Propagation\n",
    "                metrics=[\"accuracy\"]) # Accuracy performance metric\n",
    "\n",
    "# Train neural network\n",
    "history = network.fit(features_train, # Features\n",
    "                      target_train, # Target vector\n",
    "                      epochs=3, # Number of epochs\n",
    "                      verbose=0, # No output\n",
    "                      batch_size=100, # Number of observations per batch\n",
    "                      validation_data=(features_test, target_test)) # Test data\n",
    "\n",
    "# Predict classes of test set\n",
    "predicted_target = network.predict(features_test)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
