{"cells":[{"cell_type":"code","source":"---\r\ntitle: \"Allstate EDA\"\r\nauthor: \"dmi3kno\"\r\ndate: \"10 October 2016\"\r\noutput: html_document\r\n---\r\n\r\n```{r setup, include=FALSE}\r\nknitr::opts_chunk$set(echo = TRUE)\r\nlibrary(data.table)\r\nlibrary(gridExtra)\r\nlibrary(corrplot)\r\nlibrary(GGally)\r\nlibrary(ggplot2)\r\nlibrary(e1071)\r\n```\r\n\r\n## Loading and exploring the data\r\n\r\nWe will quickly load the data and look around. The competition rules state that the data contain some anonymized categorical and numerical features.\r\n\r\n```{r loading}\r\ndt_train <- fread(\"../input/train.csv\")\r\ndt_test <- fread(\"../input/test.csv\")\r\n\r\nnames(dt_train)\r\n\r\ncat_var <- names(dt_train)[which(sapply(dt_train, is.character))]\r\nnum_var <- names(dt_train)[which(sapply(dt_train, is.numeric))]\r\nnum_var <- setdiff(num_var, c(\"id\", \"loss\"))\r\n\r\ndt_train_cat <- dt_train[,.SD, .SDcols = cat_var]\r\ndt_train_num <- dt_train[,.SD,.SDcols = num_var]\r\n\r\n```\r\n\r\nIt seems there are total of `r length(cat_var)` categorical and `r length(num_var)` continuous features, not counting `id` and the label.\r\n\r\n``` {r uncomment_if_curious}\r\n#Missing values\r\n#colSums(sapply(dt_train, is.na))\r\n\r\n# Check for duplicated rows.\r\n#cat(\"The number of duplicated rows are\", nrow(dt_train) - nrow(unique(dt_train)))\r\n```\r\n\r\nThere are no missing values and no duplicated rows.\r\n\r\n## Plots\r\n\r\nLets look at some plots to see what we are up against in this competition. We will need some helper functions which I creatively borrowed [from AiO](https://www.kaggle.com/notaapple/house-prices-advanced-regression-techniques/detailed-exploratory-data-analysis-using-r/discussion)\r\n\r\n```{r helperfun}\r\n\r\nplotBox <- function(data_in, i, lab) {\r\n  data <- data.frame(x=data_in[[i]], y=lab)\r\n  p <- ggplot(data=data, aes(x=x, y=y)) +geom_boxplot()+ xlab(colnames(data_in)[i]) + theme_light() + \r\n   ylab(\"log(loss)\") + theme(axis.text.x = element_text(angle = 90, hjust =1))\r\n  return (p)\r\n}\r\n\r\ndoPlots <- function(data_in, fun, ii, lab, ncol=3) {\r\n  pp <- list()\r\n  for (i in ii) {\r\n    p <- fun(data_in=data_in, i=i, lab=lab)\r\n    pp <- c(pp, list(p))\r\n  }\r\n  do.call(\"grid.arrange\", c(pp, ncol=ncol))\r\n}\r\n\r\nplotScatter <- function(data_in, i, lab){\r\n  data <- data.frame(x=data_in[[i]], y = lab)\r\n  p <- ggplot(data= data, aes(x = x, y=y)) + geom_point(size=1, alpha=0.3)+ geom_smooth(method = lm) +\r\n    xlab(paste0(colnames(data_in)[i], '\\n', 'R-Squared: ', round(cor(data_in[[i]], lab, use = 'complete.obs'), 2)))+\r\n    ylab(\"log(loss)\") + theme_light()\r\n  return(suppressWarnings(p))\r\n} \r\n\r\nplotDen <- function(data_in, i, lab){\r\n  data <- data.frame(x=data_in[[i]], y=lab)\r\n  p <- ggplot(data= data) + geom_density(aes(x = x), size = 1,alpha = 1.0) +\r\n    xlab(paste0((colnames(data_in)[i]), '\\n', 'Skewness: ',round(skewness(data_in[[i]], na.rm = TRUE), 2))) +\r\n    theme_light() \r\n  return(p)\r\n}\r\n```\r\n\r\nNow, we are going to call these functions one after another to explore categorical and then continiuous features\r\n\r\n### Categorical features\r\n\r\nFeatures look very similar with most of them having quite low cardinality. Uncomment more lines to see the rest of features.\r\n\r\n```{r boxplots, echo=FALSE}\r\ndoPlots(dt_train_cat, fun = plotBox, ii =1:12, lab=log(dt_train$loss), ncol = 3)\r\n#doPlots(dt_train_cat, fun = plotBox, ii =13:24, lab=log(dt_train$loss), ncol = 3)\r\n#doPlots(dt_train_cat, fun = plotBox, ii =25:36, lab=log(dt_train$loss), ncol = 3)\r\n#doPlots(dt_train_cat, fun = plotBox, ii =37:48, lab=log(dt_train$loss), ncol = 3)\r\n#doPlots(dt_train_cat, fun = plotBox, ii =49:60, lab=log(dt_train$loss), ncol = 3)\r\n#doPlots(dt_train_cat, fun = plotBox, ii =61:72, lab=log(dt_train$loss), ncol = 3)\r\n#doPlots(dt_train_cat, fun = plotBox, ii =73:84, lab=log(dt_train$loss), ncol = 3)\r\n#doPlots(dt_train_cat, fun = plotBox, ii =85:96, lab=log(dt_train$loss), ncol = 3)\r\n#doPlots(dt_train_cat, fun = plotBox, ii =97:108, lab=log(dt_train$loss), ncol = 3)\r\n#doPlots(dt_train_cat, fun = plotBox, ii =109:116, lab=log(dt_train$loss), ncol = 3)\r\n```\r\n\r\n### Continuous features\r\n\r\nDensity functions for some of the continuous features make me think they used to be categorical as well.\r\n\r\n```{r density_plots, echo=FALSE}\r\ndoPlots(dt_train_num, fun = plotDen, ii =1:6, lab=log(dt_train$loss), ncol = 3)\r\ndoPlots(dt_train_num, fun = plotDen, ii =7:14, lab=log(dt_train$loss), ncol = 3)\r\n\r\n```\r\n\r\nPatient is mostly dead\r\n\r\n```{r scatter_plots, echo=FALSE}\r\ndoPlots(dt_train_num, fun = plotScatter, ii =1:6, lab=log(dt_train$loss), ncol = 3)\r\ndoPlots(dt_train_num, fun = plotScatter, ii =7:14, lab=log(dt_train$loss), ncol = 3)\r\n\r\n```\r\n\r\n## Correlations\r\n\r\nSome of the continuous features are quite correlated, which makes it hard to count on linear regressions right away\r\n\r\n```{r correlations}\r\ncorrelations <- cor(dt_train_num)\r\ncorrplot(correlations, method=\"square\", order=\"hclust\")\r\n```\r\n\r\n## Label\r\n\r\nWe need to say something about the label as well. Easiest is just to look at the histogram. Again, I am going to look at the log, because the distribution is quite skewed.\r\n\r\n```{r}\r\nggplot(dt_train) + geom_histogram(mapping=aes(x=log(loss)))\r\n```\r\n\r\nTo be continued","metadata":{"collapsed":false,"_kg_hide-input":false},"execution_count":0,"outputs":[]}],"metadata":{"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":4}