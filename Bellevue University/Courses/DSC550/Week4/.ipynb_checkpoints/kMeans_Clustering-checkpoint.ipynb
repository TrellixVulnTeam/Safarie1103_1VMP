{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.cluster import KMeansClusterer\n",
    "from sklearn.base import BaseEstimator,TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "PKL_PATTERN = r'(?!\\.)[ a-z_\\ s] +/[ a-f0-9] +\\. pickle' class PickledCorpusReader( HTMLCorpusReader): def __init__( self, root, fileids = PKL_PATTERN, ** kwargs): if not any( key.startswith(' cat_') for key in kwargs.keys()): kwargs[' cat_pattern'] = CAT_PATTERN CategorizedCorpusReader.__init__( self, kwargs) CorpusReader.__init__( self, root, fileids) def docs( self, fileids = None, categories = None): fileids = self.resolve( fileids, categories) # Load one pickled document into memory at a time. for path in self.abspaths( fileids): with open( path, 'rb') as f: yield pickle.load( f) Because each document is represented as a Python list of paragraphs,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class KmeansClsuters(BaseEstimator,TransformerMixin):\n",
    "\n",
    "    def __init__(self,k=7):\n",
    "        \"\"\"\n",
    "        K is the number of clusters. model is the implementation of kmeans \n",
    "        \"\"\"\n",
    "        self.k = k\n",
    "        self.distance = nltk.cluster.util.cosine_distance\n",
    "        self.model = KmeansClsuters(self.k,self.distance,avoid_empty_clusters=True)\n",
    "        \n",
    "    def fit(self,documents,labels=None):\n",
    "        return self\n",
    "    def transform(self,documents):\n",
    "        \"\"\"\n",
    "        Fits the K-Means model to one-hot vectorized documents \n",
    "        \"\"\"\n",
    "        return self.model.cluster(documents,assign_clusters=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class TextNormalizer( BaseEstimator, TransformerMixin):\n",
    "    def transform( self, documents): \n",
    "        return [' '. join( self.normalize( doc)) for doc in documents]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin \n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "\n",
    "class OneHotVectorizer( BaseEstimator, TransformerMixin): \n",
    "    def __init__( self): \n",
    "        self.vectorizer = CountVectorizer( binary = True) \n",
    "        \n",
    "    def fit( self, documents, labels = None): \n",
    "        return self \n",
    "        \n",
    "    def transform( self, documents): \n",
    "        freqs = self.vectorizer.fit_transform( documents) \n",
    "        return [freq.toarray()[ 0] for freq in freqs] \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PickledCorpusReader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-600316643961>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpipeline\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mcorpus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPickledCorpusReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../ corpus'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mdocs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdocs\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mcategories\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m' news'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m' norm'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTextNormalizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m' vect'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mOneHotVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m' clusters'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mKMeansClusters\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m7\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mclusters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mdocs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'PickledCorpusReader' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "corpus = PickledCorpusReader('../ corpus') \n",
    "docs = corpus.docs( categories =[' news']) \n",
    "model = Pipeline([ (' norm', TextNormalizer()), (' vect', OneHotVectorizer()), (' clusters', KMeansClusters( k = 7)) ]) \n",
    "clusters = model.fit_transform( docs) \n",
    "pickles = list( corpus.fileids( categories =[' news'])) \n",
    "for idx, cluster in enumerate( clusters): \n",
    "    print(\" Document '{}' assigned to cluster {}.\". format( pickles[ idx], cluster))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
