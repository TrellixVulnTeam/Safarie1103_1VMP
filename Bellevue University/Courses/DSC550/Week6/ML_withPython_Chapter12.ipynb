{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Chapter 12. Model Selection\n",
    "\n",
    "### 12.0 Introduction \n",
    "\n",
    "In machine learning, we use training algorithms to learn the parameters of a model by minimizing some loss function. However, in addition many learning algorithms (e.g., support vector classifier and random forests) also have hyperparameters that must be defined outside of the learning process. For example, random forests are collections of decision trees (hence the word forest); however, the number of decision trees in the forest is not learned by the algorithm and must be set prior to fitting. This is often referred to as hyperparameter tuning, hyperparameter optimization, or model selection. Additionally, often we might want to try multiple learning algorithms (for example, trying both support vector classifier and random forests to see which learning method produces the best model). \n",
    "\n",
    "While there is widespread variation in the use of terminology in this area, in this book we refer to both selecting the best learning algorithm and its best hyperparameters as model selection. The reason is straightforward: imagine we have data and want to train a support vector classifier with 10 candidate hyperparameter values and a random forest classifier with 10 candidate hyperparameter values. The result is that we are trying to select the best model from a set of 20 candidate models. In this chapter, we will cover techniques to efficiently select the best model from the set of candidates. Throughout this chapter we will refer to specific hyperparameters, such as C (the inverse of regularization strength). Do not worry if you don’t know what the hyperparameters are. We will cover them in later chapters. Instead, just treat hyperparameters like the settings for the learning algorithm that we must choose before starting training. \n",
    "\n",
    "### 12.1 Selecting Best Models Using Exhaustive Search \n",
    "\n",
    "#### Problem \n",
    "\n",
    "You want to select the best model by searching over a range of hyperparameters. \n",
    "\n",
    "#### Solution \n",
    "\n",
    "Use scikit-learn’s GridSearchCV:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "[CV] C=1.0, penalty=l1 ...............................................\n",
      "[CV] ..................... C=1.0, penalty=l1, score=nan, total=   0.0s\n",
      "[CV] C=1.0, penalty=l1 ...............................................\n",
      "[CV] ..................... C=1.0, penalty=l1, score=nan, total=   0.0s\n",
      "[CV] C=1.0, penalty=l1 ...............................................\n",
      "[CV] ..................... C=1.0, penalty=l1, score=nan, total=   0.0s\n",
      "[CV] C=1.0, penalty=l1 ...............................................\n",
      "[CV] ..................... C=1.0, penalty=l1, score=nan, total=   0.0s\n",
      "[CV] C=1.0, penalty=l1 ...............................................\n",
      "[CV] ..................... C=1.0, penalty=l1, score=nan, total=   0.0s\n",
      "[CV] C=1.0, penalty=l2 ...............................................\n",
      "[CV] ................... C=1.0, penalty=l2, score=0.967, total=   0.0s\n",
      "[CV] C=1.0, penalty=l2 ...............................................\n",
      "[CV] ................... C=1.0, penalty=l2, score=1.000, total=   0.0s\n",
      "[CV] C=1.0, penalty=l2 ...............................................\n",
      "[CV] ................... C=1.0, penalty=l2, score=0.933, total=   0.1s\n",
      "[CV] C=1.0, penalty=l2 ...............................................\n",
      "[CV] ................... C=1.0, penalty=l2, score=0.967, total=   0.1s\n",
      "[CV] C=1.0, penalty=l2 ...............................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................... C=1.0, penalty=l2, score=1.000, total=   0.1s\n",
      "[CV] C=2.7825594022071245, penalty=l1 ................................\n",
      "[CV] ...... C=2.7825594022071245, penalty=l1, score=nan, total=   0.0s\n",
      "[CV] C=2.7825594022071245, penalty=l1 ................................\n",
      "[CV] ...... C=2.7825594022071245, penalty=l1, score=nan, total=   0.0s\n",
      "[CV] C=2.7825594022071245, penalty=l1 ................................\n",
      "[CV] ...... C=2.7825594022071245, penalty=l1, score=nan, total=   0.0s\n",
      "[CV] C=2.7825594022071245, penalty=l1 ................................\n",
      "[CV] ...... C=2.7825594022071245, penalty=l1, score=nan, total=   0.0s\n",
      "[CV] C=2.7825594022071245, penalty=l1 ................................\n",
      "[CV] ...... C=2.7825594022071245, penalty=l1, score=nan, total=   0.0s\n",
      "[CV] C=2.7825594022071245, penalty=l2 ................................\n",
      "[CV] .... C=2.7825594022071245, penalty=l2, score=0.967, total=   0.1s\n",
      "[CV] C=2.7825594022071245, penalty=l2 ................................\n",
      "[CV] .... C=2.7825594022071245, penalty=l2, score=1.000, total=   0.1s\n",
      "[CV] C=2.7825594022071245, penalty=l2 ................................\n",
      "[CV] .... C=2.7825594022071245, penalty=l2, score=0.933, total=   0.0s\n",
      "[CV] C=2.7825594022071245, penalty=l2 ................................\n",
      "[CV] .... C=2.7825594022071245, penalty=l2, score=0.967, total=   0.0s\n",
      "[CV] C=2.7825594022071245, penalty=l2 ................................\n",
      "[CV] .... C=2.7825594022071245, penalty=l2, score=1.000, total=   0.1s\n",
      "[CV] C=7.742636826811269, penalty=l1 .................................\n",
      "[CV] ....... C=7.742636826811269, penalty=l1, score=nan, total=   0.0s\n",
      "[CV] C=7.742636826811269, penalty=l1 .................................\n",
      "[CV] ....... C=7.742636826811269, penalty=l1, score=nan, total=   0.0s\n",
      "[CV] C=7.742636826811269, penalty=l1 .................................\n",
      "[CV] ....... C=7.742636826811269, penalty=l1, score=nan, total=   0.0s\n",
      "[CV] C=7.742636826811269, penalty=l1 .................................\n",
      "[CV] ....... C=7.742636826811269, penalty=l1, score=nan, total=   0.0s\n",
      "[CV] C=7.742636826811269, penalty=l1 .................................\n",
      "[CV] ....... C=7.742636826811269, penalty=l1, score=nan, total=   0.0s\n",
      "[CV] C=7.742636826811269, penalty=l2 .................................\n",
      "[CV] ..... C=7.742636826811269, penalty=l2, score=0.967, total=   0.1s\n",
      "[CV] C=7.742636826811269, penalty=l2 .................................\n",
      "[CV] ..... C=7.742636826811269, penalty=l2, score=1.000, total=   0.0s\n",
      "[CV] C=7.742636826811269, penalty=l2 .................................\n",
      "[CV] ..... C=7.742636826811269, penalty=l2, score=0.967, total=   0.1s\n",
      "[CV] C=7.742636826811269, penalty=l2 .................................\n",
      "[CV] ..... C=7.742636826811269, penalty=l2, score=0.967, total=   0.1s\n",
      "[CV] C=7.742636826811269, penalty=l2 .................................\n",
      "[CV] ..... C=7.742636826811269, penalty=l2, score=1.000, total=   0.1s\n",
      "[CV] C=21.544346900318832, penalty=l1 ................................\n",
      "[CV] ...... C=21.544346900318832, penalty=l1, score=nan, total=   0.0s\n",
      "[CV] C=21.544346900318832, penalty=l1 ................................\n",
      "[CV] ...... C=21.544346900318832, penalty=l1, score=nan, total=   0.0s\n",
      "[CV] C=21.544346900318832, penalty=l1 ................................\n",
      "[CV] ...... C=21.544346900318832, penalty=l1, score=nan, total=   0.0s\n",
      "[CV] C=21.544346900318832, penalty=l1 ................................\n",
      "[CV] ...... C=21.544346900318832, penalty=l1, score=nan, total=   0.0s\n",
      "[CV] C=21.544346900318832, penalty=l1 ................................\n",
      "[CV] ...... C=21.544346900318832, penalty=l1, score=nan, total=   0.0s\n",
      "[CV] C=21.544346900318832, penalty=l2 ................................\n",
      "[CV] .... C=21.544346900318832, penalty=l2, score=1.000, total=   0.0s\n",
      "[CV] C=21.544346900318832, penalty=l2 ................................\n",
      "[CV] .... C=21.544346900318832, penalty=l2, score=1.000, total=   0.1s\n",
      "[CV] C=21.544346900318832, penalty=l2 ................................\n",
      "[CV] .... C=21.544346900318832, penalty=l2, score=0.967, total=   0.1s\n",
      "[CV] C=21.544346900318832, penalty=l2 ................................\n",
      "[CV] .... C=21.544346900318832, penalty=l2, score=0.933, total=   0.1s\n",
      "[CV] C=21.544346900318832, penalty=l2 ................................\n",
      "[CV] .... C=21.544346900318832, penalty=l2, score=1.000, total=   0.0s\n",
      "[CV] C=59.94842503189409, penalty=l1 .................................\n",
      "[CV] ....... C=59.94842503189409, penalty=l1, score=nan, total=   0.0s\n",
      "[CV] C=59.94842503189409, penalty=l1 .................................\n",
      "[CV] ....... C=59.94842503189409, penalty=l1, score=nan, total=   0.0s\n",
      "[CV] C=59.94842503189409, penalty=l1 .................................\n",
      "[CV] ....... C=59.94842503189409, penalty=l1, score=nan, total=   0.0s\n",
      "[CV] C=59.94842503189409, penalty=l1 .................................\n",
      "[CV] ....... C=59.94842503189409, penalty=l1, score=nan, total=   0.0s\n",
      "[CV] C=59.94842503189409, penalty=l1 .................................\n",
      "[CV] ....... C=59.94842503189409, penalty=l1, score=nan, total=   0.0s\n",
      "[CV] C=59.94842503189409, penalty=l2 .................................\n",
      "[CV] ..... C=59.94842503189409, penalty=l2, score=1.000, total=   0.1s\n",
      "[CV] C=59.94842503189409, penalty=l2 .................................\n",
      "[CV] ..... C=59.94842503189409, penalty=l2, score=1.000, total=   0.1s\n",
      "[CV] C=59.94842503189409, penalty=l2 .................................\n",
      "[CV] ..... C=59.94842503189409, penalty=l2, score=0.967, total=   0.1s\n",
      "[CV] C=59.94842503189409, penalty=l2 .................................\n",
      "[CV] ..... C=59.94842503189409, penalty=l2, score=0.933, total=   0.1s\n",
      "[CV] C=59.94842503189409, penalty=l2 .................................\n",
      "[CV] ..... C=59.94842503189409, penalty=l2, score=1.000, total=   0.1s\n",
      "[CV] C=166.81005372000593, penalty=l1 ................................\n",
      "[CV] ...... C=166.81005372000593, penalty=l1, score=nan, total=   0.0s\n",
      "[CV] C=166.81005372000593, penalty=l1 ................................\n",
      "[CV] ...... C=166.81005372000593, penalty=l1, score=nan, total=   0.0s\n",
      "[CV] C=166.81005372000593, penalty=l1 ................................\n",
      "[CV] ...... C=166.81005372000593, penalty=l1, score=nan, total=   0.0s\n",
      "[CV] C=166.81005372000593, penalty=l1 ................................\n",
      "[CV] ...... C=166.81005372000593, penalty=l1, score=nan, total=   0.0s\n",
      "[CV] C=166.81005372000593, penalty=l1 ................................\n",
      "[CV] ...... C=166.81005372000593, penalty=l1, score=nan, total=   0.0s\n",
      "[CV] C=166.81005372000593, penalty=l2 ................................\n",
      "[CV] .... C=166.81005372000593, penalty=l2, score=1.000, total=   0.0s\n",
      "[CV] C=166.81005372000593, penalty=l2 ................................\n",
      "[CV] .... C=166.81005372000593, penalty=l2, score=1.000, total=   0.1s\n",
      "[CV] C=166.81005372000593, penalty=l2 ................................\n",
      "[CV] .... C=166.81005372000593, penalty=l2, score=0.933, total=   0.0s\n",
      "[CV] C=166.81005372000593, penalty=l2 ................................\n",
      "[CV] .... C=166.81005372000593, penalty=l2, score=0.933, total=   0.1s\n",
      "[CV] C=166.81005372000593, penalty=l2 ................................\n",
      "[CV] .... C=166.81005372000593, penalty=l2, score=1.000, total=   0.1s\n",
      "[CV] C=464.15888336127773, penalty=l1 ................................\n",
      "[CV] ...... C=464.15888336127773, penalty=l1, score=nan, total=   0.0s\n",
      "[CV] C=464.15888336127773, penalty=l1 ................................\n",
      "[CV] ...... C=464.15888336127773, penalty=l1, score=nan, total=   0.0s\n",
      "[CV] C=464.15888336127773, penalty=l1 ................................\n",
      "[CV] ...... C=464.15888336127773, penalty=l1, score=nan, total=   0.0s\n",
      "[CV] C=464.15888336127773, penalty=l1 ................................\n",
      "[CV] ...... C=464.15888336127773, penalty=l1, score=nan, total=   0.0s\n",
      "[CV] C=464.15888336127773, penalty=l1 ................................\n",
      "[CV] ...... C=464.15888336127773, penalty=l1, score=nan, total=   0.0s\n",
      "[CV] C=464.15888336127773, penalty=l2 ................................\n",
      "[CV] .... C=464.15888336127773, penalty=l2, score=1.000, total=   0.1s\n",
      "[CV] C=464.15888336127773, penalty=l2 ................................\n",
      "[CV] .... C=464.15888336127773, penalty=l2, score=1.000, total=   0.1s\n",
      "[CV] C=464.15888336127773, penalty=l2 ................................\n",
      "[CV] .... C=464.15888336127773, penalty=l2, score=0.933, total=   0.1s\n",
      "[CV] C=464.15888336127773, penalty=l2 ................................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .... C=464.15888336127773, penalty=l2, score=0.933, total=   0.1s\n",
      "[CV] C=464.15888336127773, penalty=l2 ................................\n",
      "[CV] .... C=464.15888336127773, penalty=l2, score=1.000, total=   0.1s\n",
      "[CV] C=1291.5496650148827, penalty=l1 ................................\n",
      "[CV] ...... C=1291.5496650148827, penalty=l1, score=nan, total=   0.0s\n",
      "[CV] C=1291.5496650148827, penalty=l1 ................................\n",
      "[CV] ...... C=1291.5496650148827, penalty=l1, score=nan, total=   0.0s\n",
      "[CV] C=1291.5496650148827, penalty=l1 ................................\n",
      "[CV] ...... C=1291.5496650148827, penalty=l1, score=nan, total=   0.0s\n",
      "[CV] C=1291.5496650148827, penalty=l1 ................................\n",
      "[CV] ...... C=1291.5496650148827, penalty=l1, score=nan, total=   0.0s\n",
      "[CV] C=1291.5496650148827, penalty=l1 ................................\n",
      "[CV] ...... C=1291.5496650148827, penalty=l1, score=nan, total=   0.0s\n",
      "[CV] C=1291.5496650148827, penalty=l2 ................................\n",
      "[CV] .... C=1291.5496650148827, penalty=l2, score=1.000, total=   0.1s\n",
      "[CV] C=1291.5496650148827, penalty=l2 ................................\n",
      "[CV] .... C=1291.5496650148827, penalty=l2, score=1.000, total=   0.1s\n",
      "[CV] C=1291.5496650148827, penalty=l2 ................................\n",
      "[CV] .... C=1291.5496650148827, penalty=l2, score=0.933, total=   0.1s\n",
      "[CV] C=1291.5496650148827, penalty=l2 ................................\n",
      "[CV] .... C=1291.5496650148827, penalty=l2, score=0.933, total=   0.1s\n",
      "[CV] C=1291.5496650148827, penalty=l2 ................................\n",
      "[CV] .... C=1291.5496650148827, penalty=l2, score=1.000, total=   0.1s\n",
      "[CV] C=3593.813663804626, penalty=l1 .................................\n",
      "[CV] ....... C=3593.813663804626, penalty=l1, score=nan, total=   0.0s\n",
      "[CV] C=3593.813663804626, penalty=l1 .................................\n",
      "[CV] ....... C=3593.813663804626, penalty=l1, score=nan, total=   0.0s\n",
      "[CV] C=3593.813663804626, penalty=l1 .................................\n",
      "[CV] ....... C=3593.813663804626, penalty=l1, score=nan, total=   0.0s\n",
      "[CV] C=3593.813663804626, penalty=l1 .................................\n",
      "[CV] ....... C=3593.813663804626, penalty=l1, score=nan, total=   0.0s\n",
      "[CV] C=3593.813663804626, penalty=l1 .................................\n",
      "[CV] ....... C=3593.813663804626, penalty=l1, score=nan, total=   0.0s\n",
      "[CV] C=3593.813663804626, penalty=l2 .................................\n",
      "[CV] ..... C=3593.813663804626, penalty=l2, score=1.000, total=   0.1s\n",
      "[CV] C=3593.813663804626, penalty=l2 .................................\n",
      "[CV] ..... C=3593.813663804626, penalty=l2, score=1.000, total=   0.1s\n",
      "[CV] C=3593.813663804626, penalty=l2 .................................\n",
      "[CV] ..... C=3593.813663804626, penalty=l2, score=0.933, total=   0.0s\n",
      "[CV] C=3593.813663804626, penalty=l2 .................................\n",
      "[CV] ..... C=3593.813663804626, penalty=l2, score=0.933, total=   0.1s\n",
      "[CV] C=3593.813663804626, penalty=l2 .................................\n",
      "[CV] ..... C=3593.813663804626, penalty=l2, score=1.000, total=   0.1s\n",
      "[CV] C=10000.0, penalty=l1 ...........................................\n",
      "[CV] ................. C=10000.0, penalty=l1, score=nan, total=   0.0s\n",
      "[CV] C=10000.0, penalty=l1 ...........................................\n",
      "[CV] ................. C=10000.0, penalty=l1, score=nan, total=   0.0s\n",
      "[CV] C=10000.0, penalty=l1 ...........................................\n",
      "[CV] ................. C=10000.0, penalty=l1, score=nan, total=   0.0s\n",
      "[CV] C=10000.0, penalty=l1 ...........................................\n",
      "[CV] ................. C=10000.0, penalty=l1, score=nan, total=   0.0s\n",
      "[CV] C=10000.0, penalty=l1 ...........................................\n",
      "[CV] ................. C=10000.0, penalty=l1, score=nan, total=   0.0s\n",
      "[CV] C=10000.0, penalty=l2 ...........................................\n",
      "[CV] ............... C=10000.0, penalty=l2, score=1.000, total=   0.1s\n",
      "[CV] C=10000.0, penalty=l2 ...........................................\n",
      "[CV] ............... C=10000.0, penalty=l2, score=1.000, total=   0.1s\n",
      "[CV] C=10000.0, penalty=l2 ...........................................\n",
      "[CV] ............... C=10000.0, penalty=l2, score=0.933, total=   0.1s\n",
      "[CV] C=10000.0, penalty=l2 ...........................................\n",
      "[CV] ............... C=10000.0, penalty=l2, score=0.933, total=   0.1s\n",
      "[CV] C=10000.0, penalty=l2 ...........................................\n",
      "[CV] ............... C=10000.0, penalty=l2, score=1.000, total=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    3.0s finished\n"
     ]
    }
   ],
   "source": [
    "# Load libraries\n",
    "import numpy as np\n",
    "from sklearn import linear_model, datasets\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Load data\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "target = iris.target\n",
    "\n",
    "# Create logistic regression\n",
    "logistic = linear_model.LogisticRegression()\n",
    "\n",
    "# Create range of candidate penalty hyperparameter values\n",
    "penalty = ['l1', 'l2']\n",
    "\n",
    "# Create range of candidate regularization hyperparameter values\n",
    "C = np.logspace(0, 4, 10)\n",
    "\n",
    "# Create dictionary hyperparameter candidates\n",
    "hyperparameters = dict(C=C, penalty=penalty)\n",
    "\n",
    "# Create grid search\n",
    "gridsearch = GridSearchCV(logistic, hyperparameters, cv=5, verbose=3)\n",
    "\n",
    "# Fit grid search\n",
    "best_model = gridsearch.fit(features, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Discussion\n",
    "\n",
    "GridSearchCV is a brute-force approach to model selection using cross-validation. Specifically, a user defines sets of possible values for one or multiple hyperparameters, and then GridSearchCV trains a model using every value and/or combination of values. The model with the best performance score is selected as the best model. For example, in our solution we used logistic regression as our learning algorithm, containing two hyperparameters: C and the regularization penalty. \n",
    "\n",
    "Don’t worry if you don’t know what C and regularization mean; we cover them in the next few chapters. Just realize that C and the regularization penalty can take a range of values, which have to be specified prior to training. For C, we define 10 possible values:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.00000000e+00, 2.78255940e+00, 7.74263683e+00, 2.15443469e+01,\n",
       "       5.99484250e+01, 1.66810054e+02, 4.64158883e+02, 1.29154967e+03,\n",
       "       3.59381366e+03, 1.00000000e+04])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.logspace(0,4,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And similarly we define two possible values for the regularization penalty: ['l1', 'l2']. For each combination of C and regularization penalty values, we train the model and evaluate it using k-fold cross-validation. In our solution, we had 10 possible values of C, 2 possible values of regularization penalty, and 5 folds. They created 10 × 2 × 5 = 100 candidate models from which the best was selected. \n",
    "\n",
    "Once GridSearchCV is complete, we can see the hyperparameters of the best model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Penalty: l2\n",
      "Best C: 7.742636826811269\n"
     ]
    }
   ],
   "source": [
    "# View best hyperparameters\n",
    "print('Best Penalty:', best_model.best_estimator_.get_params()['penalty'])\n",
    "print('Best C:', best_model.best_estimator_.get_params()['C'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, after identifying the best hyperparameters, GridSearchCV will retrain a model using the best hyperparameters on the entire dataset (rather than leaving a fold out for cross-validation). We can use this model to predict values like any other scikit-learn model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict target vector\n",
    "best_model.predict(features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One GridSearchCV parameter is worth noting: verbose. While mostly unnecessary, it can be reassuring during long searching processes to receive an indication that the search is progressing. The verbose parameter determines the amount of messages outputted during the search, with 0 showing no output, and 1 to 3 outputting messages with increasing detail. \n",
    "\n",
    "#### See Also \n",
    "\n",
    "[scikit-learn documentation: GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.2 Selecting Best Models Using Randomized Search \n",
    "\n",
    "#### Problem \n",
    "\n",
    "You want a computationally cheaper method than exhaustive search to select the best model. \n",
    "\n",
    "#### Solution \n",
    "\n",
    "Use scikit-learn’s RandomizedSearchCV:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "from scipy.stats import uniform\n",
    "from sklearn import linear_model, datasets\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Load data\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "target = iris.target\n",
    "\n",
    "# Create logistic regression\n",
    "logistic = linear_model.LogisticRegression()\n",
    "\n",
    "# Create range of candidate regularization penalty hyperparameter values\n",
    "penalty = ['l1', 'l2']\n",
    "\n",
    "# Create distribution of candidate regularization hyperparameter values\n",
    "C = uniform(loc=0, scale=4)\n",
    "\n",
    "# Create hyperparameter options\n",
    "hyperparameters = dict(C=C, penalty=penalty)\n",
    "\n",
    "# Create randomized search\n",
    "randomizedsearch = RandomizedSearchCV(\n",
    "    logistic, hyperparameters, random_state=1, n_iter=100, cv=5, verbose=0,\n",
    "    n_jobs=-1)\n",
    "\n",
    "# Fit randomized search\n",
    "best_model = randomizedsearch.fit(features, target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion \n",
    "\n",
    "In Recipe 12.1, we used GridSearchCV on a user-defined set of hyperparameter values to search for the best model according to a score function. A more efficient method than GridSearchCV’s brute-force search is to search over a specific number of random combinations of hyperparameter values from user-supplied distributions (e.g., normal, uniform). scikit-learn implements this randomized search technique with RandomizedSearchCV. \n",
    "\n",
    "With RandomizedSearchCV, if we specify a distribution, scikit-learn will randomly sample without replacement hyperparameter values from that distribution. As an example of the general concept, here we randomly sample 10 values from a uniform distribution ranging from 0 to 4:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.19525402, 2.86075747, 2.4110535 , 2.17953273, 1.6946192 ,\n",
       "       2.58357645, 1.75034885, 3.567092  , 3.85465104, 1.53376608])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a uniform distribution between 0 and 4, sample 10 values\n",
    "uniform(loc=0, scale=4).rvs(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, if we specify a list of values such as two regularization penalty hyperparameter values, ['l1', 'l2'], RandomizedSearchCV will randomly sample with replacement from the list. \n",
    "\n",
    "Just like with GridSearchCV, we can see the hyperparameter values of the best model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Penalty: l2\n",
      "Best C: 3.730229437354635\n"
     ]
    }
   ],
   "source": [
    "# View best hyperparameters\n",
    "print('Best Penalty:', best_model.best_estimator_.get_params()['penalty'])\n",
    "print('Best C:', best_model.best_estimator_.get_params()['C'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And just like with GridSearchCV, after the search is complete RandomizedSearchCV fits a new model using the best hyperparameters on the entire dataset. We can use this model like any other in scikit-learn; for example, to make predictions:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict target vector\n",
    "best_model.predict(features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of sampled combinations of hyperparameters (i.e., the number of candidate models trained) is specified with the n_iter (number of iterations) setting.\n",
    "\n",
    "#### See Also \n",
    "\n",
    "[scikit-learn documentation: RandomizedSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html) \n",
    "\n",
    "[Random Search for Hyper-Parameter Optimization](http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  12.3 Selecting Best Models from Multiple Learning Algorithms \n",
    "\n",
    "#### Problem\n",
    "\n",
    "You want to select the best model by searching over a range of learning algorithms and their respective hyperparameters. \n",
    "\n",
    "#### Solution \n",
    "\n",
    "Create a dictionary of candidate learning algorithms and their hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Load libraries\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(0)\n",
    "\n",
    "# Load data\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "target = iris.target\n",
    "\n",
    "# Create a pipeline\n",
    "pipe = Pipeline([(\"classifier\", RandomForestClassifier())])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary with candidate learning algorithms and their hyperparameters\n",
    "search_space = [{\"classifier\": [LogisticRegression()],\n",
    "                 \"classifier__penalty\": ['l1', 'l2'],\n",
    "                 \"classifier__C\": np.logspace(0, 4, 10)},\n",
    "                {\"classifier\": [RandomForestClassifier()],\n",
    "                 \"classifier__n_estimators\": [10, 100, 1000],\n",
    "                 \"classifier__max_features\": [1, 2, 3]}]\n",
    "\n",
    "# Create grid search\n",
    "gridsearch = GridSearchCV(pipe, search_space, cv=5, verbose=0)\n",
    "\n",
    "# Fit grid search\n",
    "best_model = gridsearch.fit(features, target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Discussion\n",
    "\n",
    "In the previous two recipes we found the best model by searching over possible hyperparameter values of a learning algorithm. However, what if we are not certain which learning algorithm to use? Recent versions of scikit-learn allow us to include learning algorithms as part of the search space. In our solution we define a search space that includes two learning algorithms: logistic regression and random forest classifier. Each learning algorithm has its own hyperparameters, and we define their candidate values using the format classifier__[hyperparameter name]. For example, for our logistic regression, to define the set of possible values for regularization hyperparameter space, C, and potential types of regularization penalties, penalty, we create a dictionary:\n",
    "\n",
    "{'classifier': [LogisticRegression()],\n",
    " 'classifier__penalty': ['l1', 'l2'],\n",
    " 'classifier__C': np.logspace(0, 4, 10)} \n",
    " \n",
    " We can also create a similar dictionary for the random forest hyperparameters: \n",
    " \n",
    " {'classifier': [RandomForestClassifier()],\n",
    " 'classifier__n_estimators': [10, 100, 1000],\n",
    " 'classifier__max_features': [1, 2, 3]} \n",
    " \n",
    "After the search is complete, we can use best_estimator_ to view the best model’s learning algorithm and hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=7.742636826811269, class_weight=None, dual=False,\n",
       "                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                   max_iter=100, multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View best model\n",
    "best_model.best_estimator_.get_params()[\"classifier\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Just like with the last two recipes, once we have fit the model selection search, we can use this best model just like any other scikit-learn model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict target vector\n",
    "best_model.predict(features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 12.4 Selecting Best Models When Preprocessing\n",
    "\n",
    "#### Problem \n",
    "\n",
    "You want to include a preprocessing step during model selection. \n",
    "\n",
    "#### Solution \n",
    "\n",
    "Create a pipeline that includes the preprocessing step and any of its parameters:\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(0)\n",
    "\n",
    "# Load data\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "target = iris.target\n",
    "\n",
    "# Create a preprocessing object that includes StandardScaler features and PCA\n",
    "preprocess = FeatureUnion([(\"std\", StandardScaler()), (\"pca\", PCA())])\n",
    "\n",
    "# Create a pipeline\n",
    "pipe = Pipeline([(\"preprocess\", preprocess),\n",
    "                 (\"classifier\", LogisticRegression())])\n",
    "\n",
    "# Create space of candidate values\n",
    "search_space = [{\"preprocess__pca__n_components\": [1, 2, 3],\n",
    "                 \"classifier__penalty\": [\"l1\", \"l2\"],\n",
    "                 \"classifier__C\": np.logspace(0, 4, 10)}]\n",
    "\n",
    "# Create grid search\n",
    "clf = GridSearchCV(pipe, search_space, cv=5, verbose=0, n_jobs=-1)\n",
    "\n",
    "# Fit grid search\n",
    "best_model = clf.fit(features, target)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Discussion \n",
    "\n",
    "Very often we will need to preprocess our data before using it to train a model. We have to be careful to properly handle preprocessing when conducting model selection. First, GridSearchCV uses cross-validation to determine which model has the highest performance. However, in cross-validation we are in effect pretending that the fold held out, as the test set is not seen, and thus not part of fitting any preprocessing steps (e.g., scaling or standardization). For this reason, we cannot preprocess the data and then run GridSearchCV. Rather, the preprocessing steps must be a part of the set of actions taken by GridSearchCV. While this might appear complex, the reality is that scikit-learn makes it simple. FeatureUnion allows us to combine multiple preprocessing actions properly. In our solution we use FeatureUnion to combine two preprocessing steps: standardize the feature values (StandardScaler) and Principal Component Analysis (PCA). This object is called preprocess and contains both of our preprocessing steps. We then include preprocess into a pipeline with our learning algorithm. The end result is that this allows us to outsource the proper (and confusing) handling of fitting, transforming, and training the models with combinations of hyperparameters to scikit-learn. \n",
    "\n",
    "Second, some preprocessing methods have their own parameters, which often have to be supplied by the user. For example, dimensionality reduction using PCA requires the user to define the number of principal components to use to produce the transformed feature set. Ideally, we would choose the number of components that produces a model with the greatest performance for some evaluation test metric. Luckily, scikit-learn makes this easy. When we include candidate component values in the search space, they are treated like any other hyperparameter to be searched over. In our solution, we defined features__pca__n_components': [1, 2, 3] in the search space to indicate that we wanted to discover if one, two, or three principal components produced the best model. \n",
    "\n",
    "After model selection is complete, we can view the preprocessing values that produced the best model. For example, we can see the best number of principal components:\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# View best model\n",
    "best_model.best_estimator_.get_params()['preprocess__pca__n_components']\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 12.5 Speeding Up Model Selection with Parallelization \n",
    "\n",
    "#### Problem \n",
    "\n",
    "You need to speed up model selection. \n",
    "\n",
    "#### Solution\n",
    "\n",
    "Use all the cores in your machine by setting n_jobs=-1:\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import numpy as np\n",
    "from sklearn import linear_model, datasets\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Load data\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "target = iris.target\n",
    "\n",
    "# Create logistic regression\n",
    "logistic = linear_model.LogisticRegression()\n",
    "\n",
    "# Create range of candidate regularization penalty hyperparameter values\n",
    "penalty = [\"l1\", \"l2\"]\n",
    "\n",
    "# Create range of candidate values for C\n",
    "C = np.logspace(0, 4, 1000)\n",
    "\n",
    "# Create hyperparameter options\n",
    "hyperparameters = dict(C=C, penalty=penalty)\n",
    "\n",
    "# Create grid search\n",
    "gridsearch = GridSearchCV(logistic, hyperparameters, cv=5, n_jobs=-1, verbose=1)\n",
    "\n",
    "# Fit grid search\n",
    "best_model = gridsearch.fit(features, target)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Discussion \n",
    "\n",
    "In the recipes of this chapter, we have kept the number of candidate models small to make the code complete quickly. However, in the real world we will often have many thousands or tens of thousands of models to train. The end result is that it can take many hours to find the best model. To speed up the process, scikit-learn lets us train multiple models simultaneously. Without going into too much technical detail, scikit-learn can simultaneously train models up to the number of cores on the machine. Most modern laptops have four cores, so (assuming you are currently on a laptop) we can potentially train four models at the same time. This will dramatically increase the speed of our model selection process. The parameter n_jobs defines the number of models to train in parallel. In our solution, we set n_jobs to -1, which tells scikit-learn to use all cores. However, by default n_jobs is set to 1, meaning it only uses one core. To demonstrate this, if we run the same GridSearch as in the solution, but with n_jobs=1, we can see it takes significantly longer to find the best model (note that exact time will depend on your computer):\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create grid search using one core\n",
    "clf = GridSearchCV(logistic, hyperparameters, cv=5, n_jobs=1, verbose=1)\n",
    "\n",
    "# Fit grid search\n",
    "best_model = clf.fit(features, target)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 12.6 Speeding Up Model Selection Using Algorithm-Specific Methods\n",
    "\n",
    "#### Problem\n",
    "\n",
    "You need to speed up model selection. \n",
    "\n",
    "#### Solution \n",
    "\n",
    "If you are using a select number of learning algorithms, use scikit-learn’s model-specific cross-validation hyperparameter tuning. For example, LogisticRegressionCV:\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "from sklearn import linear_model, datasets\n",
    "\n",
    "# Load data\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "target = iris.target\n",
    "\n",
    "# Create cross-validated logistic regression\n",
    "logit = linear_model.LogisticRegressionCV(Cs=100)\n",
    "\n",
    "# Train model\n",
    "logit.fit(features, target)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Discussion \n",
    "\n",
    "Sometimes the characteristics of a learning algorithm allow us to search for the best hyperparameters significantly faster than either brute force or randomized model search methods. In scikit-learn, many learning algorithms (e.g., ridge, lasso, and elastic net regression) have an algorithm-specific cross-validation method to take advantage of this. For example, LogisticRegression is used to conduct a standard logistic regression classifier, while LogisticRegressionCV implements an efficient cross-validated logistic regression classifier that has the ability to identify the optimum value of the hyperparameter C. \n",
    "\n",
    "scikit-learn’s LogisticRegressionCV method includes a parameter Cs. If supplied a list, Cs is the candidate hyperparameter values to select from. If supplied an integer, the parameter Cs generates a list of that many candidate values. The candidate values are drawn logarithmically from a range between 0.0001 and 1,0000 (a range of reasonable values for C). \n",
    "\n",
    "However, a major downside to LogisticRegressionCV is that it can only search a range of values for C. In Recipe 12.1 our possible hyperparameter space included both C and another hyperparameter (the regularization penalty norm). This limitation is common to many of scikit-learn’s model-specific cross-validated approaches. \n",
    "\n",
    "#### See Also \n",
    "\n",
    "[scikit-learn documentation: LogisticRegressionCV](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegressionCV.html#sklearn.linear_model.LogisticRegressionCV)\n",
    "\n",
    "[scikit-learn documentation: Model specific cross-validation](https://scikit-learn.org/stable/modules/grid_search.html#model-specific-cross-validation)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 12.7 Evaluating Performance After Model Selection \n",
    "\n",
    "#### Problem\n",
    "\n",
    "You want to evaluate the performance of a model found through model selection. \n",
    "\n",
    "#### Solution \n",
    "\n",
    "Use nested cross-validation to avoid biased evaluation:\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import numpy as np\n",
    "from sklearn import linear_model, datasets\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "\n",
    "# Load data\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "target = iris.target\n",
    "\n",
    "# Create logistic regression\n",
    "logistic = linear_model.LogisticRegression()\n",
    "\n",
    "# Create range of 20 candidate values for C\n",
    "C = np.logspace(0, 4, 20)\n",
    "\n",
    "# Create hyperparameter options\n",
    "hyperparameters = dict(C=C)\n",
    "\n",
    "# Create grid search\n",
    "\n",
    "gridsearch = GridSearchCV(logistic, hyperparameters, cv=5, n_jobs=-1, verbose=0)\n",
    "\n",
    "# Conduct nested cross-validation and outut the average score\n",
    "cross_val_score(gridsearch, features, target).mean()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Discussion \n",
    "\n",
    "Nested cross-validation during model selection is a difficult concept for many people to grasp the first time. Remember that in k-fold cross-validation, we train our model on k–1 folds of the data, use this model to make predictions on the remaining fold, and then evaluate our model best on how well our model’s predictions compare to the true values. We then repeat this process k times. In the model selection searches described in this chapter (i.e., GridSearchCV and RandomizedSearchCV), we used cross-validation to evaluate which hyperparameter values produced the best models. However, a nuanced and generally underappreciated problem arises: since we used the data to select the best hyperparameter values, we cannot use that same data to evaluate the model’s performance. The solution? Wrap the cross-validation used for model search in another cross-validation! In nested cross-validation, the “inner” cross-validation selects the best model, while the “outer” cross-validation provides us with an unbiased evaluation of the model’s performance. In our solution, the inner cross-validation is our GridSearchCV object, which we then wrap in an outer cross-validation using cross_val_score.\n",
    "\n",
    "If you are confused, try a simple experiment. First, set verbose=1 so we can see what is happening:\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gridsearch = GridSearchCV(logistic, hyperparameters, cv=5, verbose=1)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, run gridsearch.fit(features, target), which is our inner cross-validation used to find the best model:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "best_model = gridsearch.fit(features, target)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "From the output you can see the inner cross-validation trained 20 candidate models five times, totaling 100 models. Next, nest clf inside a new cross-validation, which defaults to three folds:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "scores = cross_val_score(gridsearch, features, target)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The output shows that the inner cross-validation trained 20 models five times to find the best model, and this model was evaluated using an outer three-fold cross-validation, creating a total of 300 models trained."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(0)\n",
    "\n",
    "# Load data\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "target = iris.target\n",
    "\n",
    "# Create a preprocessing object that includes StandardScaler features and PCA\n",
    "preprocess = FeatureUnion([(\"std\", StandardScaler()), (\"pca\", PCA())])\n",
    "\n",
    "# Create a pipeline\n",
    "pipe = Pipeline([(\"preprocess\", preprocess),\n",
    "                 (\"classifier\", LogisticRegression())])\n",
    "\n",
    "# Create space of candidate values\n",
    "search_space = [{\"preprocess__pca__n_components\": [1, 2, 3],\n",
    "                 \"classifier__penalty\": [\"l1\", \"l2\"],\n",
    "                 \"classifier__C\": np.logspace(0, 4, 10)}]\n",
    "\n",
    "# Create grid search\n",
    "clf = GridSearchCV(pipe, search_space, cv=5, verbose=0, n_jobs=-1)\n",
    "\n",
    "# Fit grid search\n",
    "best_model = clf.fit(features, target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion \n",
    "\n",
    "Very often we will need to preprocess our data before using it to train a model. We have to be careful to properly handle preprocessing when conducting model selection. First, GridSearchCV uses cross-validation to determine which model has the highest performance. However, in cross-validation we are in effect pretending that the fold held out, as the test set is not seen, and thus not part of fitting any preprocessing steps (e.g., scaling or standardization). For this reason, we cannot preprocess the data and then run GridSearchCV. Rather, the preprocessing steps must be a part of the set of actions taken by GridSearchCV. While this might appear complex, the reality is that scikit-learn makes it simple. FeatureUnion allows us to combine multiple preprocessing actions properly. In our solution we use FeatureUnion to combine two preprocessing steps: standardize the feature values (StandardScaler) and Principal Component Analysis (PCA). This object is called preprocess and contains both of our preprocessing steps. We then include preprocess into a pipeline with our learning algorithm. The end result is that this allows us to outsource the proper (and confusing) handling of fitting, transforming, and training the models with combinations of hyperparameters to scikit-learn. \n",
    "\n",
    "Second, some preprocessing methods have their own parameters, which often have to be supplied by the user. For example, dimensionality reduction using PCA requires the user to define the number of principal components to use to produce the transformed feature set. Ideally, we would choose the number of components that produces a model with the greatest performance for some evaluation test metric. Luckily, scikit-learn makes this easy. When we include candidate component values in the search space, they are treated like any other hyperparameter to be searched over. In our solution, we defined features__pca__n_components': [1, 2, 3] in the search space to indicate that we wanted to discover if one, two, or three principal components produced the best model. \n",
    "\n",
    "After model selection is complete, we can view the preprocessing values that produced the best model. For example, we can see the best number of principal components:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View best model\n",
    "best_model.best_estimator_.get_params()['preprocess__pca__n_components']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.5 Speeding Up Model Selection with Parallelization \n",
    "\n",
    "#### Problem \n",
    "\n",
    "You need to speed up model selection. \n",
    "\n",
    "#### Solution\n",
    "\n",
    "Use all the cores in your machine by setting n_jobs=-1:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2000 candidates, totalling 10000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done 1200 tasks      | elapsed:    5.6s\n",
      "[Parallel(n_jobs=-1)]: Done 3200 tasks      | elapsed:   15.9s\n",
      "[Parallel(n_jobs=-1)]: Done 6000 tasks      | elapsed:   30.6s\n",
      "[Parallel(n_jobs=-1)]: Done 9600 tasks      | elapsed:   51.9s\n",
      "[Parallel(n_jobs=-1)]: Done 9985 out of 10000 | elapsed:   54.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 10000 out of 10000 | elapsed:   54.2s finished\n"
     ]
    }
   ],
   "source": [
    "# Load libraries\n",
    "import numpy as np\n",
    "from sklearn import linear_model, datasets\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Load data\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "target = iris.target\n",
    "\n",
    "# Create logistic regression\n",
    "logistic = linear_model.LogisticRegression()\n",
    "\n",
    "# Create range of candidate regularization penalty hyperparameter values\n",
    "penalty = [\"l1\", \"l2\"]\n",
    "\n",
    "# Create range of candidate values for C\n",
    "C = np.logspace(0, 4, 1000)\n",
    "\n",
    "# Create hyperparameter options\n",
    "hyperparameters = dict(C=C, penalty=penalty)\n",
    "\n",
    "# Create grid search\n",
    "gridsearch = GridSearchCV(logistic, hyperparameters, cv=5, n_jobs=-1, verbose=1)\n",
    "\n",
    "# Fit grid search\n",
    "best_model = gridsearch.fit(features, target)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion \n",
    "\n",
    "In the recipes of this chapter, we have kept the number of candidate models small to make the code complete quickly. However, in the real world we will often have many thousands or tens of thousands of models to train. The end result is that it can take many hours to find the best model. To speed up the process, scikit-learn lets us train multiple models simultaneously. Without going into too much technical detail, scikit-learn can simultaneously train models up to the number of cores on the machine. Most modern laptops have four cores, so (assuming you are currently on a laptop) we can potentially train four models at the same time. This will dramatically increase the speed of our model selection process. The parameter n_jobs defines the number of models to train in parallel. In our solution, we set n_jobs to -1, which tells scikit-learn to use all cores. However, by default n_jobs is set to 1, meaning it only uses one core. To demonstrate this, if we run the same GridSearch as in the solution, but with n_jobs=1, we can see it takes significantly longer to find the best model (note that exact time will depend on your computer):\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2000 candidates, totalling 10000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 10000 out of 10000 | elapsed:  5.0min finished\n"
     ]
    }
   ],
   "source": [
    "# Create grid search using one core\n",
    "clf = GridSearchCV(logistic, hyperparameters, cv=5, n_jobs=1, verbose=1)\n",
    "\n",
    "# Fit grid search\n",
    "best_model = clf.fit(features, target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.6 Speeding Up Model Selection Using Algorithm-Specific Methods\n",
    "\n",
    "#### Problem\n",
    "\n",
    "You need to speed up model selection. \n",
    "\n",
    "#### Solution \n",
    "\n",
    "If you are using a select number of learning algorithms, use scikit-learn’s model-specific cross-validation hyperparameter tuning. For example, LogisticRegressionCV:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegressionCV(Cs=100, class_weight=None, cv=None, dual=False,\n",
       "                     fit_intercept=True, intercept_scaling=1.0, l1_ratios=None,\n",
       "                     max_iter=100, multi_class='auto', n_jobs=None,\n",
       "                     penalty='l2', random_state=None, refit=True, scoring=None,\n",
       "                     solver='lbfgs', tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load libraries\n",
    "from sklearn import linear_model, datasets\n",
    "\n",
    "# Load data\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "target = iris.target\n",
    "\n",
    "# Create cross-validated logistic regression\n",
    "logit = linear_model.LogisticRegressionCV(Cs=100)\n",
    "\n",
    "# Train model\n",
    "logit.fit(features, target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion \n",
    "\n",
    "Sometimes the characteristics of a learning algorithm allow us to search for the best hyperparameters significantly faster than either brute force or randomized model search methods. In scikit-learn, many learning algorithms (e.g., ridge, lasso, and elastic net regression) have an algorithm-specific cross-validation method to take advantage of this. For example, LogisticRegression is used to conduct a standard logistic regression classifier, while LogisticRegressionCV implements an efficient cross-validated logistic regression classifier that has the ability to identify the optimum value of the hyperparameter C. \n",
    "\n",
    "scikit-learn’s LogisticRegressionCV method includes a parameter Cs. If supplied a list, Cs is the candidate hyperparameter values to select from. If supplied an integer, the parameter Cs generates a list of that many candidate values. The candidate values are drawn logarithmically from a range between 0.0001 and 1,0000 (a range of reasonable values for C). \n",
    "\n",
    "However, a major downside to LogisticRegressionCV is that it can only search a range of values for C. In Recipe 12.1 our possible hyperparameter space included both C and another hyperparameter (the regularization penalty norm). This limitation is common to many of scikit-learn’s model-specific cross-validated approaches. \n",
    "\n",
    "#### See Also \n",
    "\n",
    "[scikit-learn documentation: LogisticRegressionCV](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegressionCV.html#sklearn.linear_model.LogisticRegressionCV)\n",
    "\n",
    "[scikit-learn documentation: Model specific cross-validation](https://scikit-learn.org/stable/modules/grid_search.html#model-specific-cross-validation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.7 Evaluating Performance After Model Selection \n",
    "\n",
    "#### Problem\n",
    "\n",
    "You want to evaluate the performance of a model found through model selection. \n",
    "\n",
    "#### Solution \n",
    "\n",
    "Use nested cross-validation to avoid biased evaluation:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9800000000000001"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load libraries\n",
    "import numpy as np\n",
    "from sklearn import linear_model, datasets\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "\n",
    "# Load data\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "target = iris.target\n",
    "\n",
    "# Create logistic regression\n",
    "logistic = linear_model.LogisticRegression()\n",
    "\n",
    "# Create range of 20 candidate values for C\n",
    "C = np.logspace(0, 4, 20)\n",
    "\n",
    "# Create hyperparameter options\n",
    "hyperparameters = dict(C=C)\n",
    "\n",
    "# Create grid search\n",
    "\n",
    "gridsearch = GridSearchCV(logistic, hyperparameters, cv=5, n_jobs=-1, verbose=0)\n",
    "\n",
    "# Conduct nested cross-validation and outut the average score\n",
    "cross_val_score(gridsearch, features, target).mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion \n",
    "\n",
    "Nested cross-validation during model selection is a difficult concept for many people to grasp the first time. Remember that in k-fold cross-validation, we train our model on k–1 folds of the data, use this model to make predictions on the remaining fold, and then evaluate our model best on how well our model’s predictions compare to the true values. We then repeat this process k times. In the model selection searches described in this chapter (i.e., GridSearchCV and RandomizedSearchCV), we used cross-validation to evaluate which hyperparameter values produced the best models. However, a nuanced and generally underappreciated problem arises: since we used the data to select the best hyperparameter values, we cannot use that same data to evaluate the model’s performance. The solution? Wrap the cross-validation used for model search in another cross-validation! In nested cross-validation, the “inner” cross-validation selects the best model, while the “outer” cross-validation provides us with an unbiased evaluation of the model’s performance. In our solution, the inner cross-validation is our GridSearchCV object, which we then wrap in an outer cross-validation using cross_val_score.\n",
    "\n",
    "If you are confused, try a simple experiment. First, set verbose=1 so we can see what is happening:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch = GridSearchCV(logistic, hyperparameters, cv=5, verbose=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, run gridsearch.fit(features, target), which is our inner cross-validation used to find the best model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    5.9s finished\n"
     ]
    }
   ],
   "source": [
    "best_model = gridsearch.fit(features, target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the output you can see the inner cross-validation trained 20 candidate models five times, totaling 100 models. Next, nest clf inside a new cross-validation, which defaults to three folds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    5.5s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    5.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    5.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    5.7s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    5.6s finished\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(gridsearch, features, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output shows that the inner cross-validation trained 20 models five times to find the best model, and this model was evaluated using an outer three-fold cross-validation, creating a total of 300 models trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}